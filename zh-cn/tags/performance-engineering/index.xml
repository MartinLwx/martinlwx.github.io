<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Performance-Engineering - 标签 - MartinLwx&#39;s Blog</title>
        <link>https://martinlwx.github.io/zh-cn/tags/performance-engineering/</link>
        <description>Performance-Engineering - 标签 - MartinLwx&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>martinlwx@163.com (MartinLwx)</managingEditor>
            <webMaster>martinlwx@163.com (MartinLwx)</webMaster><copyright>&lt;a rel=&#34;license noopener&#34; href=&#34;https://creativecommons.org/licenses/by-nc-nd/4.0/&#34; target=&#34;_blank&#34;&gt;CC BY-NC-ND 4.0&lt;/a&gt;</copyright><lastBuildDate>Wed, 18 Jun 2025 23:07:55 &#43;0800</lastBuildDate><atom:link href="https://martinlwx.github.io/zh-cn/tags/performance-engineering/" rel="self" type="application/rss+xml" /><item>
    <title>异步编程&#43;漏桶算法：批量调用 LLM API</title>
    <link>https://martinlwx.github.io/zh-cn/async-and-leaky-bucket-algorithm-batch-llm-api-call/</link>
    <pubDate>Wed, 18 Jun 2025 23:07:55 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/async-and-leaky-bucket-algorithm-batch-llm-api-call/</guid>
    <description><![CDATA[<h2 id="背景" class="headerLink">
    <a href="#%e8%83%8c%e6%99%af" class="header-mark"></a>背景</h2><p>最近在工作中着手模型评测平台的搭建，其中有这么一个场景：需要调用其他部门提供的 LLM API 进行在评测集上跑模型评测，但这个 LLM API 有请求速率限制 - 最多 1 秒调用 2 次（2 RPS）。所以我的任务概括来说就是：<em>如何在严格遵守 API 速率请求的情况下，最大提高并发度加快模型评测速度</em>。本文的内容主要记录了对这个任务的尝试，以及最后的解决方案</p>]]></description>
</item><item>
    <title>Python 的 2 个进程池相关 API</title>
    <link>https://martinlwx.github.io/zh-cn/data-parallel-with-two-different-api/</link>
    <pubDate>Sun, 30 Mar 2025 21:36:18 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/data-parallel-with-two-different-api/</guid>
    <description><![CDATA[<h2 id="intro" class="headerLink">
    <a href="#intro" class="header-mark"></a>Intro</h2><p>作为一名算法工程师，在我的工作中经常都会写各种 Python 脚本来处理大量数据，做数据清洗、信息提取等。通常情况下，这些数据的处理并不涉及竞争条件（Race Condition），而是简单的数据并行（Data Parallel），属于 <em>CPU 密集型任务</em>。通常情况下，这样的任务可以被抽象为 <code>map(fn, data)</code> 的模式</p>]]></description>
</item><item>
    <title>尾调用与尾调用优化</title>
    <link>https://martinlwx.github.io/zh-cn/tail-call-and-tail-call-optimization/</link>
    <pubDate>Fri, 22 Mar 2024 12:13:53 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/tail-call-and-tail-call-optimization/</guid>
    <description><![CDATA[<h2 id="尾调用--尾递归" class="headerLink">
    <a href="#%e5%b0%be%e8%b0%83%e7%94%a8--%e5%b0%be%e9%80%92%e5%bd%92" class="header-mark"></a>尾调用 &amp; 尾递归</h2><p>假设函数 <code>A</code> 调用了函数 <code>B</code>，我们称函数 <code>A</code> 为 <em>Caller</em>，函数 <code>B</code> 为 <em>Callee</em>。</p>
<p>尾调用（Tail-call）指的是：Caller <em>最后只需要</em>返回 Callee 这个函数调用的计算结果，其他运算<em>都</em>执行完成了<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>]]></description>
</item></channel>
</rss>
