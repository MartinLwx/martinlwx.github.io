<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Machine-Learning - 标签 - MartinLwx&#39;s Blog</title>
        <link>https://martinlwx.github.io/zh-cn/tags/machine-learning/</link>
        <description>Machine-Learning - 标签 - MartinLwx&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>martinlwx@163.com (MartinLwx)</managingEditor>
            <webMaster>martinlwx@163.com (MartinLwx)</webMaster><copyright>&lt;a rel=&#34;license noopener&#34; href=&#34;https://creativecommons.org/licenses/by-nc-nd/4.0/&#34; target=&#34;_blank&#34;&gt;CC BY-NC-ND 4.0&lt;/a&gt;</copyright><lastBuildDate>Wed, 30 Jul 2025 23:03:58 &#43;0800</lastBuildDate><atom:link href="https://martinlwx.github.io/zh-cn/tags/machine-learning/" rel="self" type="application/rss+xml" /><item>
    <title>关联规则挖掘：Apriori 算法</title>
    <link>https://martinlwx.github.io/zh-cn/use-apriori-algorithm-for-association-rule-mining/</link>
    <pubDate>Wed, 30 Jul 2025 23:03:58 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/use-apriori-algorithm-for-association-rule-mining/</guid>
    <description><![CDATA[<h2 id="引言" class="headerLink">
    <a href="#%e5%bc%95%e8%a8%80" class="header-mark"></a>引言</h2><p>最近的工作中需要分析大量安卓 Apk 的特征关联，这些特征包括 IP 特征、URL 特征、权限特征等。特征关联指的是从数据中挖掘哪些特征之间存在关联，比如某些特征组合经常一起出现</p>
<p>如果依靠人工经验的话，数据量太大不大现实，就在这时我想起了以前数据挖掘课程上学过的一个算法：Apriori 算法 :)</p>]]></description>
</item><item>
    <title>神奇的 torch.einsum API</title>
    <link>https://martinlwx.github.io/zh-cn/the-magic-torch-einsum-api/</link>
    <pubDate>Mon, 14 Apr 2025 20:44:01 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/the-magic-torch-einsum-api/</guid>
    <description><![CDATA[<h2 id="motivations" class="headerLink">
    <a href="#motivations" class="header-mark"></a>Motivations</h2><p>在 PyTorch 里面存在着<em>很多</em>跟矩阵乘法、矩阵向量乘法等操作相关的 API，这对记忆来说是一种负担。并且，在使用这些 API 的过程中<em>经常</em>需要对矩阵进行 reshape 等操作，确保维度信息对得上</p>]]></description>
</item><item>
    <title>KNN 算法是什么</title>
    <link>https://martinlwx.github.io/zh-cn/what-is-k-nearest-neighbor-algorithms/</link>
    <pubDate>Sun, 15 Dec 2024 15:10:00 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/what-is-k-nearest-neighbor-algorithms/</guid>
    <description><![CDATA[<h2 id="什么是-knn" class="headerLink">
    <a href="#%e4%bb%80%e4%b9%88%e6%98%af-knn" class="header-mark"></a>什么是 KNN</h2><div class="details admonition tip open">
    <div class="details-summary admonition-title">
        <span class="icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 352 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M96.06 454.35c.01 6.29 1.87 12.45 5.36 17.69l17.09 25.69a31.99 31.99 0 0 0 26.64 14.28h61.71a31.99 31.99 0 0 0 26.64-14.28l17.09-25.69a31.989 31.989 0 0 0 5.36-17.69l.04-38.35H96.01l.05 38.35zM0 176c0 44.37 16.45 84.85 43.56 115.78 16.52 18.85 42.36 58.23 52.21 91.45.04.26.07.52.11.78h160.24c.04-.26.07-.51.11-.78 9.85-33.22 35.69-72.6 52.21-91.45C335.55 260.85 352 220.37 352 176 352 78.61 272.91-.3 175.45 0 73.44.31 0 82.97 0 176zm176-80c-44.11 0-80 35.89-80 80 0 8.84-7.16 16-16 16s-16-7.16-16-16c0-61.76 50.24-112 112-112 8.84 0 16 7.16 16 16s-7.16 16-16 16z"/></svg></span>Tip<span class="details-icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></span>
    </div>
    <div class="details-content">
        <div class="admonition-content"><p>显然，从定义来看，KNN 算法<em>并不需要训练</em></p>]]></description>
</item><item>
    <title>TF-IDF 模型</title>
    <link>https://martinlwx.github.io/zh-cn/an-introduction-of-tf-idf-model/</link>
    <pubDate>Wed, 16 Aug 2023 22:23:26 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/an-introduction-of-tf-idf-model/</guid>
    <description><![CDATA[<h2 id="什么是-tf-idf-模型" class="headerLink">
    <a href="#%e4%bb%80%e4%b9%88%e6%98%af-tf-idf-%e6%a8%a1%e5%9e%8b" class="header-mark"></a>什么是 TF-IDF 模型</h2><p>在之前的 <a href="https://martinlwx.github.io/zh-cn/an-introduction-of-bag-of-word-model/" rel="">文章</a> 中谈到了词袋模型，也讲到了它的许多不足，在今天的这篇文章中，我们要尝试解决词袋模型的<em>缺点</em>之一：每个词的重要性是一样的</p>
<div class="details admonition quesstion open">
    <div class="details-summary admonition-title">
        <span class="icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M497.9 142.1l-46.1 46.1c-4.7 4.7-12.3 4.7-17 0l-111-111c-4.7-4.7-4.7-12.3 0-17l46.1-46.1c18.7-18.7 49.1-18.7 67.9 0l60.1 60.1c18.8 18.7 18.8 49.1 0 67.9zM284.2 99.8L21.6 362.4.4 483.9c-2.9 16.4 11.4 30.6 27.8 27.8l121.5-21.3 262.6-262.6c4.7-4.7 4.7-12.3 0-17l-111-111c-4.8-4.7-12.4-4.7-17.1 0zM124.1 339.9c-5.5-5.5-5.5-14.3 0-19.8l154-154c5.5-5.5 14.3-5.5 19.8 0s5.5 14.3 0 19.8l-154 154c-5.5 5.5-14.3 5.5-19.8 0zM88 424h48v36.3l-64.5 11.3-31.1-31.1L51.7 376H88v48z"/></svg></span>Quesstion<span class="details-icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></span>
    </div>
    <div class="details-content">
        <div class="admonition-content"><p>那么，核心问题就是————如何定义「单词的重要性」这个概念？</p>]]></description>
</item><item>
    <title>词袋模型</title>
    <link>https://martinlwx.github.io/zh-cn/an-introduction-of-bag-of-word-model/</link>
    <pubDate>Fri, 11 Aug 2023 18:55:09 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/an-introduction-of-bag-of-word-model/</guid>
    <description><![CDATA[<h2 id="什么是词袋模型" class="headerLink">
    <a href="#%e4%bb%80%e4%b9%88%e6%98%af%e8%af%8d%e8%a2%8b%e6%a8%a1%e5%9e%8b" class="header-mark"></a>什么是词袋模型</h2><p>在 NLP 中，我们需要将文档（document）表示为向量，这是因为机器学习只能够处理数字。也就是说，我们要找到下面这么一个<em>神奇</em>的函数：</p>
<p>$$
f(\text{document}) = vector
$$</p>
<p>今天要讨论的是词袋模型（bag-of-word, BoW），词袋模型可以让我们把输入的文档转变成一个向量表示</p>]]></description>
</item><item>
    <title>机器学习求解梯度的小技巧</title>
    <link>https://martinlwx.github.io/zh-cn/a-trick-to-calculating-partial-derivatives-in-ml/</link>
    <pubDate>Wed, 26 Jul 2023 00:31:50 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/a-trick-to-calculating-partial-derivatives-in-ml/</guid>
    <description><![CDATA[<h2 id="引言" class="headerLink">
    <a href="#%e5%bc%95%e8%a8%80" class="header-mark"></a>引言</h2><p>也许你和我一样在求解机器学习的梯度时有各种困难，即使看着相关的 <a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf" target="_blank" rel="noopener noreferrer">Cookbook</a> 一边推导依然是有困惑，今天我要分享的是最近<a href="https://youtu.be/JLg1HkzDsKI" target="_blank" rel="noopener noreferrer">学习到</a>的一个实用技巧：<strong>在机器学习中，求解偏导数的时候可以先全部看成标量处理，最后让维度匹配即可</strong></p>]]></description>
</item><item>
    <title>线性回归模型指南 - 理论部分</title>
    <link>https://martinlwx.github.io/zh-cn/linear-regression-model-guide-theory/</link>
    <pubDate>Wed, 15 Mar 2023 12:27:40 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/linear-regression-model-guide-theory/</guid>
    <description><![CDATA[<h2 id="引言" class="headerLink">
    <a href="#%e5%bc%95%e8%a8%80" class="header-mark"></a>引言</h2><p>最近，重新刷起了吴恩达的机器学习课程，系统性复习了之前学过的知识，发现又有不少收获，打算仔细整理一番👍</p>
<p>要谈论什么是线性回归首先要对什么是机器学习有一个基本的认识，什么是机器学习？抽象来说机器学习就是学习一个函数
$$
f(input) = output
$$
其中 $f$ 指的就是具体的机器学习模型。<strong>机器学习就是自动拟合输入 - 输出之间的关系</strong>的一套方法论。有时候我们会发现一些问题很难定义出一个具体的算法来解决，这时就是机器学习发光发热的地方了，我们可以让它从数据中自己学习、总结一些模式，做出相关的预测。这也是它和传统的算法（二分、递归等）区别的地方。不得不承认，机器学习从定义上来说就很迷人，它<em>似乎</em>为所有难以解决的问题提供了一套可行的解决框架。恰恰现实生活中的一些问题就是很难用传统算法解决的</p>]]></description>
</item><item>
    <title>从混淆矩阵到 F1 score</title>
    <link>https://martinlwx.github.io/zh-cn/f1score/</link>
    <pubDate>Sun, 05 Dec 2021 21:25:19 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/f1score/</guid>
    <description><![CDATA[<h2 id="什么是混淆矩阵" class="headerLink">
    <a href="#%e4%bb%80%e4%b9%88%e6%98%af%e6%b7%b7%e6%b7%86%e7%9f%a9%e9%98%b5" class="header-mark"></a>什么是混淆矩阵</h2><hr>
<p><strong>每一列表示实际情况，每一行表示我们的预测</strong>，这样组合起来就得到了一个混淆矩阵，比如一个二分类的任务，可以画出如下的混淆矩阵⬇️</p>
<center>
<table>
  <thead>
      <tr>
          <th></th>
          <th>Positive</th>
          <th>Negative</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>True</td>
          <td>TP = True Positive</td>
          <td>FP = False Positive</td>
      </tr>
      <tr>
          <td>False</td>
          <td>FN = False Negative</td>
          <td>TN = True Negative</td>
      </tr>
  </tbody>
</table>
</center>
<h3 id="accuracy" class="headerLink">
    <a href="#accuracy" class="header-mark"></a>accuracy</h3><blockquote>
  <p>有多少样本我们预测正确了</p>]]></description>
</item></channel>
</rss>
