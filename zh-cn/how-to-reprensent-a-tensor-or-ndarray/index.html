<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>Pytorch 张量的 strides 格式是什么 - MartinLwx&#39;s blog</title><meta name="Description" content="介绍了 Pytorch 的张量的 strides 格式原理"><meta property="og:title" content="Pytorch 张量的 strides 格式是什么" />
<meta property="og:description" content="介绍了 Pytorch 的张量的 strides 格式原理" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://martinlwx.github.io/zh-cn/how-to-reprensent-a-tensor-or-ndarray/" /><meta property="og:image" content="https://martinlwx.github.io/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-14T15:26:16+08:00" />
<meta property="article:modified_time" content="2023-07-14T15:26:16+08:00" /><meta property="og:site_name" content="MartinLwx&#39;s Blog" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://martinlwx.github.io/logo.png"/>

<meta name="twitter:title" content="Pytorch 张量的 strides 格式是什么"/>
<meta name="twitter:description" content="介绍了 Pytorch 的张量的 strides 格式原理"/>
<meta name="application-name" content="我的网站">
<meta name="apple-mobile-web-app-title" content="我的网站"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://martinlwx.github.io/zh-cn/how-to-reprensent-a-tensor-or-ndarray/" /><link rel="prev" href="https://martinlwx.github.io/zh-cn/how-to-memorize-insertion-and-deletion-in-rb-tree/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="/lib/fontawesome-free/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"></noscript><link rel="preload" href="/lib/animate/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="/lib/animate/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Pytorch 张量的 strides 格式是什么",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/martinlwx.github.io\/zh-cn\/how-to-reprensent-a-tensor-or-ndarray\/"
        },"genre": "posts","keywords": "Internal, Pytorch, Deep-Learning","wordcount":  4007 ,
        "url": "https:\/\/martinlwx.github.io\/zh-cn\/how-to-reprensent-a-tensor-or-ndarray\/","datePublished": "2023-07-14T15:26:16+08:00","dateModified": "2023-07-14T15:26:16+08:00","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "MartinLwx"
            },"description": "介绍了 Pytorch 的张量的 strides 格式原理"
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/zh-cn/" title="MartinLwx&#39;s blog"></a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/zh-cn/"> 主页 </a><a class="menu-item" href="/zh-cn/posts/"> 文章 </a><a class="menu-item" href="/zh-cn/tags/"> 标签 </a><a class="menu-item" href="/zh-cn/categories/"> 分类 </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a><a href="javascript:void(0);" class="menu-item language" title="选择语言">
                    <i class="fa fa-globe" aria-hidden="true"></i>                      
                    <select class="language-select" id="language-select-desktop" onchange="location = this.value;"><option value="/en/how-to-reprensent-a-tensor-or-ndarray/">English</option><option value="/zh-cn/how-to-reprensent-a-tensor-or-ndarray/" selected>简体中文</option></select>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/zh-cn/" title="MartinLwx&#39;s blog"></a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><a class="menu-item" href="/zh-cn/" title="">主页</a><a class="menu-item" href="/zh-cn/posts/" title="">文章</a><a class="menu-item" href="/zh-cn/tags/" title="">标签</a><a class="menu-item" href="/zh-cn/categories/" title="">分类</a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a><a href="javascript:void(0);" class="menu-item" title="选择语言">
                    <i class="fa fa-globe fa-fw" aria-hidden="true"></i>
                    <select class="language-select" onchange="location = this.value;"><option value="/en/how-to-reprensent-a-tensor-or-ndarray/">English</option><option value="/zh-cn/how-to-reprensent-a-tensor-or-ndarray/" selected>简体中文</option></select>
                </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">目录</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Pytorch 张量的 strides 格式是什么</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/zh-cn/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>MartinLwx</a></span>&nbsp;<span class="post-category">收录于 <a href="/zh-cn/categories/ml-dl/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>ML-DL</a>&nbsp;<a href="/zh-cn/categories/internal/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Internal</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2023-07-14">2023-07-14</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;约 4007 字&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;预计阅读 8 分钟&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="true">
                <div class="details-summary toc-title">
                    <span>目录</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#引言">引言</a></li>
    <li><a href="#按行存储与按列存储">按行存储与按列存储</a></li>
    <li><a href="#strides-格式">Strides 格式</a></li>
    <li><a href="#why-strides">Why strides？</a>
      <ul>
        <li><a href="#print_internal-函数">print_internal 函数</a></li>
        <li><a href="#permute-操作">permute 操作</a></li>
        <li><a href="#broadcast_to-操作">broadcast_to 操作</a></li>
        <li><a href="#reshape-操作和-contiguous-操作">reshape 操作和 contiguous 操作</a></li>
      </ul>
    </li>
    <li><a href="#总结">总结</a></li>
    <li><a href="#参考">参考</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><h2 id="引言">引言</h2>
<p>尽管我已经使用 Numpy 和 Pytorch 好长一段时间了，但我一直不知道他们是如何实现底层的张量（tensor），而且<strong>这么高效</strong>。最近在看 <a href="https://dlsyscourse.org/" target="_blank" rel="noopener noreffer ">Deep Learning Systems</a> 这门课，终于有机会尝试自己实现张量，实现一遍之后对张量的理解更深刻了🧐</p>
<p>作为 Pytorch 的使用者有必要理解底层的张量存储原理吗？我觉得是<strong>有必要的</strong>，大多数情况下理解底层原理都能让你更好理解上层的东西，<em>比如理解张量的存储原理之后有助于你会回答下面这几个问题</em></p>
<ul>
<li>广播操作涉及到数组的拷贝吗？</li>
<li>Pytorch 的 <code>contiguous</code> 中是干什么的？为什么需要这个函数？</li>
</ul>
<h2 id="按行存储与按列存储">按行存储与按列存储</h2>
<p>让我们从简单的二维数组出发，<strong>二维数组在内存中占据连续的位置</strong>，但是要按行存储还是按列存储这点可能不相同</p>
<p><em>比如现在有下面这个 $2\times 3$ 的二维数组 <code>A</code></em></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="p">[[</span><span class="mf">0.2949</span><span class="p">,</span> <span class="mf">0.9608</span><span class="p">,</span> <span class="mf">0.0965</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"> <span class="p">[</span><span class="mf">0.5463</span><span class="p">,</span> <span class="mf">0.4176</span><span class="p">,</span> <span class="mf">0.8146</span><span class="p">]]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>如果是按行存储</strong>，那么内存中的排列（这里记为 <code>A_in_row</code>）是：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="p">[</span><span class="mf">0.2949</span><span class="p">,</span> <span class="mf">0.9608</span><span class="p">,</span> <span class="mf">0.0965</span><span class="p">,</span> <span class="mf">0.5463</span><span class="p">,</span> <span class="mf">0.4176</span><span class="p">,</span> <span class="mf">0.8146</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>按行存储的时候，要访问 <code>(i, j)</code> 位置的值的公式是</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">A_in_row</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">j</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>如果是按列存储，那么内存中的排列（这里记为 <code>A_in_col</code>）是：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="p">[</span><span class="mf">0.2949</span><span class="p">,</span> <span class="mf">0.5463</span><span class="p">,</span> <span class="mf">0.9608</span><span class="p">,</span> <span class="mf">0.4176</span><span class="p">,</span> <span class="mf">0.0965</span><span class="p">,</span> <span class="mf">0.8146</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>按列存储的时候，要访问 <code>(i, j)</code> 位置的值的公式是</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">A_in_col</span><span class="p">[</span><span class="n">j</span> <span class="o">*</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="strides-格式">Strides 格式</h2>
<p>张量在底层可以是按行存储也可以是按列存储。Numpy 和 Pytorch 都采用了<strong>按行存储</strong>的方式，<strong>任何维度的张量在底层存储都占据着内存中连续的空间</strong>，那么问题来了，我们如何访问到我们想要的位置的数据？</p>
<p>答案就是 <strong>strides 格式</strong>。strides 格式可以看成是前面两种索引格式的泛化版本，假设现在有一个 $N$ 维的张量 <code>A</code>（假设维度从 0 开始），它的底层存储为 <code>A_internal</code>，我们想要访问 <code>A[i0][i1][i2]...</code>，那么索引的方式如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">A</span><span class="p">[</span><span class="n">i0</span><span class="p">][</span><span class="n">i1</span><span class="p">][</span><span class="n">i2</span><span class="p">]</span><span class="o">...</span> <span class="o">=</span> <span class="n">A_internal</span><span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_offset</span>
</span></span><span class="line"><span class="cl">    <span class="o">+</span> <span class="n">i0</span> <span class="o">*</span> <span class="n">A</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="o">+</span> <span class="n">i1</span> <span class="o">*</span> <span class="n">A</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="o">+</span> <span class="n">i2</span> <span class="o">*</span> <span class="n">A</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="o">+</span> <span class="o">...</span>
</span></span><span class="line"><span class="cl">    <span class="o">+</span> <span class="ow">in</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">A</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>Strides 格式有两个组成部分</strong></p>
<ul>
<li><code>offset</code> - 表示张量相对于底层存储 <code>A_internal</code> 的偏移量</li>
<li><code>strides</code> 数组，<strong>长度和张量的维度一样</strong>，<code>strides[i]</code> 表示张量在第 $i$ 个维度上移动“一个单位”需要在内存上跳过多少个元素</li>
</ul>
<p><em>举例来说，前面提到的二维数组的例子，如果用 strides 的格式来理解的话，应该是下面这样</em></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">A_in_row</span><span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="o">+</span> <span class="n">j</span> <span class="o">*</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>即对于一个大小为 <code>(A.shape[0], A.shape[1])</code> 的二维数组，它的 <code>offset</code> 是 <code>0</code>，<code>strides = [A.shape[1], 1]</code>(row-major)。🤔️ <em>也就是说，每次在第一个维度上要跳跃“一个单位”，需要跳过底层的 <code>A.shape[1]</code> 个元素，<code>A.shape[1]</code> 也就是行的长度</em></p>
<blockquote>
<p>我做了下面这张图片，希望能够帮助你理解 :)</p>
</blockquote>
<figure><img src="/img/2D_tensor_strides.png" width="resize"/>
</figure>

<p>🧐 那么如何得到 $N$ 维张量的 strides 数组？假设要求解的是 <code>strides[k]</code> 即第 $k$ 个维度的 stride，我们知道它的语义是「在第 $k$ 维上移动“一个单位”需要在内存上跳过多少个元素」，<strong>如果这个张量的底层存储在内存上是连续存储（紧凑格式）</strong>，那就是 「$k+1,k+2,&hellip;,N-1$ 维度的大小的乘积」，如果 $k=N-1$，那么 <code>strides[N - 1] = 1</code></p>
<p>数学公式就是下面这样，
$$strides[k]=\prod_{i=k+1}^{N-1}shape[i]$$</p>
<blockquote>
<p>💡 再次强调，上面的公式只有在张量的底层存储在内存上是连续存储（紧凑格式）的时候成立</p>
</blockquote>
<h2 id="why-strides">Why strides？</h2>
<p>知道了 strides 的存储格式之后，<strong>我们还要理解为什么这么设计</strong>，strides 究竟给我们带来了什么？<strong>最大的好处是：很多关于张量的操作都可以是零拷贝（Zero-copy）的</strong>。通过 strides 格式，<strong>「底层存储」和「视图」之间是分离开的</strong>，下面我来讲解一下几个常见的操作</p>
<h3 id="print_internal-函数">print_internal 函数</h3>
<p>在开始之前，让我们先写一个函数获取 Pytorch 的张量底层存储</p>
<p>首先是 Pytorch 提供的 <code>data_ptr()</code> 这个方法，他会返回张量底层存储表示的第一个元素的内存地址</p>
<p>然后通过 Pytorch 提供的 <code>storage().nbytes()</code> 就可以知道张量的底层存储在内存上占据了多大的空间<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>，而张量的 <code>dtype</code> 属性则告诉了我们每个元素占据多大，<em>比如 <code>torch.float32</code> 就是 4 个字节</em></p>
<p>最后通过 <code>ctypes.string_at(address, size=-1)</code> 函数就可以读取这个张量为 C 的字符串（buffer），而 <code>torch.frombuffer</code> 可以从一个 buffer 创建出 tensor</p>
<p>通过上面几个步骤，我们就可以还原出 Pytorch 底层的数组表示，下面命名为 <code>print_internal</code> 函数</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">print_internal</span><span class="p">(</span><span class="n">t</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">torch</span><span class="o">.</span><span class="n">frombuffer</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">ctypes</span><span class="o">.</span><span class="n">string_at</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">(),</span> <span class="n">t</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span><span class="o">.</span><span class="n">nbytes</span><span class="p">()),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">t</span><span class="o">.</span><span class="n">dtype</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>然后我们创建一个维度为 <code>(1, 2, 3, 4)</code> 的张量 <code>t</code> 并观察它的底层表示，<strong>后面的操作讲解会基于这个张量 <code>t</code></strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">24</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[[[ 0,  1,  2,  3],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#           [ 4,  5,  6,  7],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#           [ 8,  9, 10, 11]],</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#          [[12, 13, 14, 15],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#           [16, 17, 18, 19],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#           [20, 21, 22, 23]]]])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (24, 12, 4, 1)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">print_internal</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([0,  1,  2,  3,</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         4,  5,  6,  7,</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         8,  9, 10, 11,</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         12, 13, 14, 15,</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         16, 17, 18, 19,</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         20, 21, 22, 23])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>按照我们前面说的从张量的维度推导 stride 的方法，我们不难知道这个 tensor 的 stride 应该是 <code>(2 * 3 * 4, 3 * 4, 4, 1)</code> 也就是 <code>(24, 12, 4, 1)</code></p>
<p>在 Pytorch 里面，我们可以通过调用 tensor 的 <code>stride()</code> 方法获得 stride，可以看到，确实跟我们手动计算出来的一样🤔️</p>
<h3 id="permute-操作">permute 操作</h3>
<p>假设我们用 <code>permute</code> 重新排列了各个维度，那么 <code>strides</code> 如何变化？</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (24, 12, 4, 1)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">permute</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="c1"># True</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">permute</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (12, 4, 1, 24)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">print_internal</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">permute</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([0,  1,  2,  3,</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         4,  5,  6,  7,</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         8,  9, 10, 11,</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         12, 13, 14, 15,</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         16, 17, 18, 19,</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         20, 21, 22, 23])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>permute</code> 操作显然不会影响 <code>offset</code>，<strong>而且底层存储仍然是紧凑的</strong>，所以我们可以通过 <code>permute</code> 之后的新的维度 <code>new_shape</code> 然后根据定义计算出 <code>strides</code>，但是<strong>更快的办法是，直接在 <code>strides</code> 上也做一样的 <code>permute</code> 操作即可</strong>。<code>print_internal</code> 函数的输出证明了底层存储确实没有变化。</p>
<h3 id="broadcast_to-操作">broadcast_to 操作</h3>
<p>广播操作是比较有意思的，在不了解张量的存储原理之前，你可能以为广播操作就是在对应的维度上拷贝多份，但其实，<strong>根本就没有发生拷贝，只是修改了 <code>strides</code> 数组的值而已</strong>。更确切来说，Pytorch 会把被广播的维度（本来的维度大小是 1）的 stride 设置为 0<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></p>
<p><em>比如现在我们在第一个维度上做广播，观察广播之后的维度大小，以及 strides 数组的变化情况</em></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="c1"># False</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># torch.Size([2, 2, 3, 4])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (24, 12, 4, 1)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (0, 12, 4, 1)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">print_internal</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([0,  1,  2,  3,</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         4,  5,  6,  7,</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         8,  9, 10, 11,</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         12, 13, 14, 15,</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         16, 17, 18, 19,</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         20, 21, 22, 23])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>你（可能）会惊讶地发现</strong>，Pytorch 确实没有在广播的时候拷贝对应维度的张量，仅仅只是修改 <code>strides</code> 数组了而已。回忆 <code>strides[i]</code> 的含义，被广播的维度的 stride 设置为 <code>0</code> 意味着这个维度上移动“一个单位“并不需要在内存上跳过元素，<strong>也就是在被广播的维度上我们一直在访问的是同一块区域</strong></p>
<h3 id="reshape-操作和-contiguous-操作">reshape 操作和 contiguous 操作</h3>
<p>索引操作可能会修改 <code>offset</code>，因为索引之后<em>形成的</em>张量不一定从本来底层存储的第一个元素开始，同时索引操作可能会索引到<strong>底层存储中的「非连续」部分</strong>。因此我们可以通过索引操作来研究 <code>reshape</code> 操作和 <code>contiguous</code> 操作是如何起作用的</p>
<p><em>现在假设我们想要从 <code>t</code> 拿到下面这个张量</em></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="p">[[[</span><span class="mi">2</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">   <span class="mi">6</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">   <span class="mi">10</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">  <span class="p">[</span><span class="mi">14</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="mi">18</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="mi">22</span><span class="p">]]]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><em>对应的索引操作如下</em></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">2</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[[ 2,  6, 10],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#          [14, 18, 22]]])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>注意到这个操作同时符合我前面说的：</p>
<ul>
<li><code>offset</code> 改变了，因为现在是从 <code>2</code> 而不是从 <code>0</code> 开始了</li>
<li>索引到的元素在本来的内存上不是连续的</li>
</ul>
<p><em>下面的代码验证了我们的猜想</em></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 2</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="c1"># False</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><em>现在来观察底层存储</em></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">print_internal</span><span class="p">(</span><span class="n">t</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">2</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([        2,  3,</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         4,  5,  6,  7,</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         8,  9, 10, 11,</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         12, 13, 14, 15,</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         16, 17, 18, 19,</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         20, 21, 22, 23</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         1152921504606846976, -8070441752123218147]])</span>
</span></span><span class="line"><span class="cl"><span class="c1"># ignore the last row because t.data_ptr() has changed but t.storage().nbytes()</span>
</span></span><span class="line"><span class="cl"><span class="c1"># kept the same.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># As a result, we read 2 invalid elements and get 2 meaningless values</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><em>Pytorch 的张量有个方法叫做 <code>storage_offset</code> 可以拿到张量相对于底层存储的偏移量，可以看到现在从底层存储的第二个位置开始了，第二个位置恰好是 <code>t[:, :, :, 2]</code> 的第一个元素 <code>2</code>。而打印出底层存储你会发现，底层存储还是本来的数组</em></p>
<blockquote>
<p>注意这里有个小问题，因为底层存储没有变化，<code>t.storage().nbytes()</code> 跟原来一样。但是 <code>data_ptr()</code> 会给我们第二个元素的地址，导致最后 <code>print_internal</code> 打印底层存储的时候会多打印 2 个无效的位置（也就是上面的最后一行），所以得到了 2 个没有意义的数字</p>
</blockquote>
<p>🤔️ <em>这个时候我们尝试执行 <code>reshape(3, 2)</code> 并观察底层存储情况</em></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">print_internal</span><span class="p">(</span><span class="n">t</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([        2,  3,</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         4,  5,  6,  7,</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         8,  9, 10, 11,</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         12, 13, 14, 15,</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         16, 17, 18, 19,</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         20, 21, 22, 23</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         1152921504606846976, -8070441752123218147]])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><em><code>reshape</code> 操作之后发现底层存储还是没有变化</em>，这恰好对应文档里面所说的：可能的情况下，<code>reshape</code> 之后，返回的张量<strong>尽可能</strong>是同一份存储<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></p>
<p><em>但如果我们想要 <code>reshape</code> 之后的张量在底层的存储是紧凑的呢？此时就可以紧跟着调用 <code>contiguous</code> 方法</em></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">print_internal</span><span class="p">(</span><span class="n">t</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([ 2,  6, 10, 14, 18, 22])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>😺 可以发现，<code>contiguous</code> 之后确实底层存储就紧凑了，此时的 strides 数组应该符合我们前面提到的公式：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># before contiguous</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (8, 4)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># after contiguous</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (3, 2)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (2, 1)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>🧐 一个比较<strong>有挑战性</strong>的问题，索引操作会如何影响 <code>strides</code>？</p>
<p><em>让我们以刚才的索引操作为例子，首先，索引之后得到新的维度应该是 <code>(1, 2, 3)</code>，显然 <code>[:, :, :, 2]</code> 这样的索引导致底层存储在内存上不紧凑，因此规律不适用，那么只能从定义上出发，假设 <code>t[:, :, :, 2]</code> 的 strides 是 <code>[x, y, z]</code></em></p>
<p><em>先观察 <code>t[:, :, :, 2]</code> 包含哪一些元素</em></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">2</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[[ 2,  6, 10],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#          [14, 18, 22]]])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>因为我定义的张量是从 0 开始的整数，因此我们可以直接观察值的变化来计算 strides 的变化</strong>（这是一个小技巧）</p>
<ul>
<li>对于 <code>z</code>，从 <code>2 -&gt; 6 -&gt; 10</code>，每次跳过了 4 个位置，所以 <code>z = 4</code></li>
<li>对于 <code>y</code>，<code>2 -&gt; 14</code>，<code>6 -&gt; 18</code>，<code>10 -&gt; 22</code>，每次都跳过了 12 个位置，因此 <code>y = 12</code></li>
<li>对于 <code>x</code>，因为<strong>底层存储并没有改变</strong>，原本的张量 <code>t</code> 的 <code>stride[0] = 24</code>，<strong>如果张量 <code>t</code> 的第一个维度不是 1 而是一个更大的值，我们还是每次会跳过 <code>stride[0]</code> 个元素</strong>，所以 <code>x = 24</code></li>
</ul>
<p>所以 <code>t[:, :, :, 2]</code> 的 <code>strides</code> 应该是 <code>(24, 12, 4)</code></p>
<p><em>让我们来调用一下 API 看这是否正确</em></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (24, 12, 4, 1)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (24, 12, 4)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># what if the first dimension is not 1 but 2?</span>
</span></span><span class="line"><span class="cl"><span class="n">another_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">48</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">another_t</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">2</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[[ 2,  6, 10],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#          [14, 18, 22]],</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#         [[26, 30, 34],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#          [38, 42, 46]]])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># you can see that 2 -&gt; 26, 6 -&gt; 30, 10 -&gt; 35</span>
</span></span><span class="line"><span class="cl"><span class="c1"># , so the stride[0] = 24 is true</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><em>上面的代码验证了我们的猜想</em></p>
<p>但是，索引操作可能远远比我们这里讲解的 <code>[:, :, :, 2]</code> 复杂得多，<em>比如 <code>[2, 1:3, 1:6:3]</code> 这种</em>，此时 <code>strides</code> 和 <code>offset</code> 又该如何变化？这里不展开，但是可以放一个<strong>提示</strong>：把每个格式都变成 Python 的 Slice 对象，然后从 <code>strides[i]</code> 的定义出发进行推导</p>
<h2 id="总结">总结</h2>
<p>可以看到，Pytorch 的张量的不少操作都是通过改变 strides 的 <code>offset</code> 或（和）<code>strides</code> 数组实现的，<strong>这让很多操作维持了零拷贝开销，因此效率会很高</strong>，而且，这使得我们可以把不少张量操作实现为 Lazy 的。理解 strides 格式<strong>有助于构建张量的 mental model，它能够让你更好理解张量的操作的代码</strong>。顺便推荐一下这个<a href="https://youtu.be/7kclgMIcMq0" target="_blank" rel="noopener noreffer ">视频</a>，在这个视频中，可以看到如何操纵 strides 来实现高效的卷积操作</p>
<p>现在我们可以回答前面我抛出的问题了：</p>
<ul>
<li>广播操作涉及到数组的拷贝吗？
<ul>
<li>并没有拷贝，只是修改了 <code>strides</code> 数组</li>
</ul>
</li>
<li>Pytorch 的 <code>contiguous</code> 中是干什么的？为什么需要这个函数？
<ul>
<li>因为 <code>contiguous</code> 之后，张量的底层存储是内存紧凑的，虽然有拷贝的开销，但是后续执行一些张量相关的操作的时候<strong>内存局部性会更好</strong></li>
</ul>
</li>
</ul>
<h2 id="参考">参考</h2>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://discuss.pytorch.org/t/tensor-type-memory-usage/150933" target="_blank" rel="noopener noreffer ">Tensor type memory usage - Memory Format - PyTorch Forums</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p><a href="https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html#torch.Tensor.expand" target="_blank" rel="noopener noreffer ">torch.expand  - PyTorch 2.0 documentation</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p><a href="https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape" target="_blank" rel="noopener noreffer ">torch.reshape — PyTorch 2.0 documentation</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>更新于 2023-07-14</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://martinlwx.github.io/zh-cn/how-to-reprensent-a-tensor-or-ndarray/" data-title="Pytorch 张量的 strides 格式是什么" data-hashtags="Internal,Pytorch,Deep-Learning"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://martinlwx.github.io/zh-cn/how-to-reprensent-a-tensor-or-ndarray/" data-hashtag="Internal"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Hacker News" data-sharer="hackernews" data-url="https://martinlwx.github.io/zh-cn/how-to-reprensent-a-tensor-or-ndarray/" data-title="Pytorch 张量的 strides 格式是什么"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="https://martinlwx.github.io/zh-cn/how-to-reprensent-a-tensor-or-ndarray/" data-title="Pytorch 张量的 strides 格式是什么"><i data-svg-src="/lib/simple-icons/icons/line.min.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://martinlwx.github.io/zh-cn/how-to-reprensent-a-tensor-or-ndarray/" data-title="Pytorch 张量的 strides 格式是什么"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/zh-cn/tags/internal/">Internal</a>,&nbsp;<a href="/zh-cn/tags/pytorch/">Pytorch</a>,&nbsp;<a href="/zh-cn/tags/deep-learning/">Deep-Learning</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/zh-cn/">主页</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/zh-cn/how-to-memorize-insertion-and-deletion-in-rb-tree/" class="prev" rel="prev" title="如何记忆红黑树的操作"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>如何记忆红黑树的操作</a></div>
</div>
</article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">由 <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.115.3">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden="true"></i> LoveIt</a>
                </div><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2019 - 2023</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/zh-cn/" target="_blank">MartinLwx</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script type="text/javascript" src="/lib/autocomplete/autocomplete.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.stemmer.support.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.zh.min.js"></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="/lib/sharer/sharer.min.js"></script><script type="text/javascript" src="/lib/katex/katex.min.js"></script><script type="text/javascript" src="/lib/katex/contrib/auto-render.min.js"></script><script type="text/javascript" src="/lib/katex/contrib/copy-tex.min.js"></script><script type="text/javascript" src="/lib/katex/contrib/mhchem.min.js"></script><script type="text/javascript" src="/lib/cookieconsent/cookieconsent.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":50},"comment":{},"cookieconsent":{"content":{"dismiss":"同意","link":"了解更多","message":"本网站使用 Cookies 来改善您的浏览体验."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"highlightTag":"em","lunrIndexURL":"/zh-cn/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"lunr"}};</script><script type="text/javascript" src="/js/theme.min.js"></script><script type="text/javascript">
            window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());
            gtag('config', 'G-7RY6742J2F', { 'anonymize_ip': true });
        </script><script type="text/javascript" src="https://www.googletagmanager.com/gtag/js?id=G-7RY6742J2F" async></script></body>
</html>
