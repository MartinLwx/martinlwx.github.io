# BPE åˆ†è¯è§£å¯† - å®ç°æ–¹æ³•ä¸ç¤ºä¾‹è®²è§£


## BPE ç®€ä»‹

åœ¨ NLP é‡Œé¢ï¼Œä¸€ä¸ªæ ¸å¿ƒçš„é—®é¢˜æ˜¯ï¼Œå¦‚ä½•å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Ÿä»åˆ†ç±»çš„è§’åº¦ä¸Šé¢æ¥è¯´ï¼Œå¯ä»¥åˆ†ä¸ºï¼š
- Char level
- Word level
- Subword level

å…ˆçœ‹ Char level åˆ†è¯ï¼Œé¡¾åæ€ä¹‰ï¼Œå°±æ˜¯æŠŠæ–‡æœ¬æ‹†åˆ†æˆä¸€ä¸ªä¸ªå­—ç¬¦å•ç‹¬è¡¨ç¤ºï¼Œ*æ¯”å¦‚ `highest -> h, i, g, h, e, s, t`*ï¼Œä¸€ä¸ªæ˜¾ç„¶çš„å¥½å¤„æ˜¯ï¼ŒVocab ä¸ä¼šå¤ªå¤§ï¼ŒVocab çš„å¤§å°ä¸ºå­—ç¬¦é›†çš„å¤§å°ï¼Œä¹Ÿä¸ä¼šé‡åˆ° Out-of-vocabulary(OOV) çš„é—®é¢˜ï¼Œä½†æ˜¯**å­—ç¬¦æœ¬èº«å¹¶æ²¡æœ‰ä¼ è¾¾å¤ªå¤šçš„è¯­ä¹‰**ï¼Œè€Œä¸”**åˆ†è¯ä¹‹åä¼šæœ‰å¤ªå¤šçš„ token**ï¼Œ*å…‰æ˜¯ä¸€ä¸ª highest å°±å¯ä»¥å¾—åˆ° 7 ä¸ª tokenï¼Œéš¾ä»¥æƒ³è±¡å¾ˆé•¿çš„æ–‡æœ¬åˆ†å‡ºæ¥ä¼šæœ‰å¤šå°‘ä¸ª*ğŸ˜¨

å†çœ‹ Word level åˆ†è¯ï¼ŒWord level åˆ†è¯ä¸€èˆ¬é€šè¿‡ç©ºæ ¼æˆ–è€…æ ‡ç‚¹ç¬¦å·æ¥æŠŠæ–‡æœ¬åˆ†æˆä¸€ä¸ªä¸ªå•è¯ï¼Œè¿™æ ·åˆ†è¯ä¹‹åçš„ token æ•°é‡å°±ä¸ä¼šå¤ªå¤šï¼Œ*æ¯”å¦‚ `Today is a good day` -> `Today, is, a, good, day`*ã€‚ä½† Word level åˆ†è¯ä¹Ÿæœ‰é—®é¢˜ï¼Œ*æ¯”å¦‚è‹±æ–‡ä¸­çš„ `high, higher, highest` è¿™ä¸‰ä¸ªå•è¯æ˜¾ç„¶è¯­ä¹‰ç›¸ä¼¼ï¼Œå› ä¸ºå¦å¤–ä¸¤ä¸ªåªæ˜¯æ¯”è¾ƒçº§ï¼Œä½†æ˜¯ Word level åˆ†è¯ä¼šæŠŠä»–ä»¬çœ‹æˆ 3 ä¸ªå•ç‹¬çš„å•è¯*

ğŸ¤”ï¸ é‚£ä¹ˆæ˜¯å¦å­˜åœ¨ä¸€ç§æŠ˜ä¸­çš„åŠæ³•ï¼Œä½¿å¾—æˆ‘ä»¬*å¤§æ¦‚ç‡*ä¸ä¼šé‡åˆ° OOV çš„é—®é¢˜ï¼›åˆ†è¯å¾—åˆ°çš„ token æ•°é‡åˆä¸èƒ½å¤ªå¤šï¼›è€Œä¸”åˆ†è¯èƒ½å¤Ÿè€ƒè™‘åˆ°å¤åˆè¯ã€æ—¶æ€å˜åŒ–ã€å•å¤æ•°å‘¢ï¼Ÿè¿™å°±æ˜¯ Subword level åšçš„äº‹æƒ…ï¼Œ*ä»ç„¶æ˜¯åˆšæ‰çš„ä¾‹å­ï¼Œæ ¹æ®è‹±è¯­çš„è¯æ ¹åŸç†çš„è¯ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠ `higher` åˆ’åˆ†ä¸º `high, er`ï¼Œ`highest` åˆ’åˆ†ä¸º `highest`*ï¼Œæ‰€ä»¥ **Subword level åˆ†è¯å°±æ˜¯æŠŠä¸€ä¸ªå•è¯ç”¨å¤šä¸ªå­è¯è¡¨ç¤º**ï¼Œä»Šå¤©è¦ä»‹ç»çš„ BPE å°±å±äº Subword level çš„ä¸€ç§

BPE çš„å…¨ç§°æ˜¯ **B**yte **P**air **E**ncodingï¼Œè¿™é‡Œæœ‰ä¸ªç»†èŠ‚å€¼å¾—æ€è€ƒï¼Œ**ä»€ä¹ˆæ˜¯ Byte pair**ï¼ŸASCII ç¼–ç çš„è¯ä»»ä½•å­—ç¬¦éƒ½æ˜¯ 1 byteï¼Œä½†å¦‚æœæ˜¯ utf-8 ç¼–ç å‘¢ï¼Ÿä¸€ä¸ªå­—ç¬¦ä¸ä¸€å®šæ˜¯ 1 byteï¼Œå®ƒå¯ä»¥æ˜¯ 3 bytes ä¹Ÿå¯ä»¥æ˜¯ 4 bytesï¼ŒğŸ¤”ï¸ é‚£å¦‚æœ BPE ç”¨åœ¨ utf-8 çš„æ–‡æœ¬é‡Œé¢ï¼Œbyte pair åˆæ˜¯ä»€ä¹ˆä¸œè¥¿ï¼Ÿæ‰€ä»¥æˆ‘æ„Ÿè§‰è¿™é‡Œæ˜¯æœ‰ç‚¹æ­§ä¹‰æ€§çš„ï¼Œå› æ­¤**æ›´å¥½çš„ç†è§£æ–¹å¼ä¹Ÿè®¸æ˜¯ï¼Œbyte pair å…¶å®æ˜¯ char pairï¼Œè¿™é‡Œçš„ char æ˜¯ utf-8 çš„ char**

### BPE è®­ç»ƒæµç¨‹

ç„¶åå¼€å§‹æ­£å¼ä»‹ç» BPE ç®—æ³•çš„è®­ç»ƒæµç¨‹ï¼Œå‡è®¾æˆ‘ä»¬æ‰‹å¤´æœ‰ä¸€å †æ–‡æ¡£ $D$
1. æŠŠæ¯ä¸ªæ–‡æ¡£ $d$ å˜æˆä¸€ä¸ªä¸ªå•è¯ï¼Œ*æ¯”å¦‚ä½ å¯ä»¥ç®€å•ç”¨ç©ºæ ¼åˆ†è¯å°±å¥½*
2. ç»Ÿè®¡æ¯ä¸ªå•è¯ $w$ åœ¨æ‰€æœ‰æ–‡æ¡£ä¸­çš„å‡ºç°é¢‘ç‡ï¼Œå¹¶å¾—åˆ°åˆå§‹çš„å­—ç¬¦é›† `alphabet` ä½œä¸ºä¸€å¼€å§‹çš„ Vocabï¼ˆåŒ…æ‹¬åé¢çš„ `</w>`ï¼‰
3. å…ˆå°†æ¯ä¸ªå•è¯åˆ’åˆ†ä¸ºä¸€ä¸ªä¸ª utf-8 charï¼Œç§°ä¸ºä¸€ä¸ªåˆ’åˆ†ï¼Œ*æ¯”å¦‚ `highest -> h, i, g, h, e, s, t`*
4. ç„¶åï¼Œåœ¨æ¯ä¸ªå•è¯çš„åˆ’åˆ†æœ€åé¢åŠ ä¸Š `</w>`ï¼Œ*é‚£ä¹ˆç°åœ¨ `highest -> h, i, g, h, e, s, t, </w>`*
5. é‡å¤ä¸‹é¢æ­¥éª¤ç›´åˆ°æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶ä¸­çš„ä»»æ„ä¸€ä¸ªï¼š1ï¼‰Vocab è¾¾åˆ°ä¸Šé™ã€‚2ï¼‰è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
    1. æ‰¾åˆ°**æœ€ç»å¸¸ä¸€èµ·å‡ºç°çš„ pair**ï¼Œå¹¶è®°å½•è¿™ä¸ªåˆå¹¶è§„åˆ™æ”¾åœ¨ merge table é‡Œé¢ï¼ŒåŒæ—¶æŠŠåˆå¹¶ä¹‹åçš„ç»“æœæ”¾åˆ° Vocab é‡Œé¢
    2. æ›´æ–°æ‰€æœ‰å•è¯çš„åˆ’åˆ†ï¼Œ*å‡è®¾æˆ‘ä»¬å‘ç° `(h, i)` æœ€ç»å¸¸ä¸€èµ·å‡ºç°ï¼Œé‚£ä¹ˆ `highest -> hi, g, h, e, s, t, </w>`*

ä½ å¯èƒ½ä¼šæœ‰ä¸‹é¢ 3 ä¸ªç–‘æƒ‘ï¼š
1. ä¸ºä»€ä¹ˆè¦ç»Ÿè®¡è¯é¢‘ï¼Ÿå› ä¸ºç»Ÿè®¡è¯é¢‘ä¼šè®©æ‰¾æœ€ç»å¸¸å‡ºç°çš„ pair æ›´ç®€å•
2. ä¸ºä»€ä¹ˆè¦åŠ  `</w>`ï¼Ÿå› ä¸ºæˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿè¿˜åŸè¾“å…¥ï¼Œå› æ­¤éœ€è¦åšä¸ªæ ‡è®°è¡¨ç¤ºè¿™æ˜¯å•è¯ä¹‹é—´çš„è¾¹ç•Œ
3. å¦‚æœå¤šä¸ª pair çš„è¯é¢‘ä¸€æ ·æ€ä¹ˆå¤„ç†ï¼Ÿè¿™ä¸ªä¸åŒå®ç°å¯èƒ½ä¸ä¸€æ ·ï¼Œä½†åœ¨æˆ‘çœ‹æ¥*åº”è¯¥*å…³ç³»ä¸å¤§

> ğŸ’¡ ç”±æ­¤ä½ å¯ä»¥å‘ç°ï¼ŒBPE ç®—æ³•åˆå¹¶æœ€ç»å¸¸ä¸€èµ·å‡ºç°çš„ pair çš„æ—¶å€™ï¼Œå¹¶ä¸ä¼šè·¨è¶Šå•è¯

### BPE åº”ç”¨æµç¨‹

åœ¨ BPE å®Œæˆè®­ç»ƒä¹‹åï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°ä¸€ä¸ª merge tableï¼Œä¹Ÿä¼šå¾—åˆ°ä¸€ä¸ª Vocabï¼Œå‡è®¾ç°åœ¨è¦å¤„ç†æ–‡æœ¬ `s`

1. å’Œè®­ç»ƒçš„æ—¶å€™é‡‡ç”¨ä¸€æ ·çš„æ–¹æ³•ï¼Œå…ˆæŠŠ `s` æ‹†åˆ†æˆä¸€ä¸ªä¸ªå•è¯ï¼Œæ¯ä¸ªå•è¯æ‹†åˆ†ä¸ºä¸€ä¸ªä¸ª utf-8 char
2. éå† merge tableï¼Œå¹¶æ£€æŸ¥æ¯ä¸ªåˆå¹¶è§„åˆ™æ˜¯å¦å¯ä»¥ç”¨æ¥æ›´æ–°æ¯ä¸ªå•è¯çš„åˆ’åˆ†ï¼Œå¯ä»¥çš„è¯å°±åˆå¹¶æ›´æ–°

> ğŸ’¡ è¿™é‡Œæœ‰ä¸ªç»†èŠ‚ï¼Œæˆ‘ä»¬æå–çš„ merge rule å…¶å®æ˜¯æŒ‰ç…§å‡ºç°çš„é¢‘ç‡é™åºï¼Œé‚£ä¹ˆæˆ‘ä»¬**æŒ‰é¡ºåºéå† merge table å°±å·²ç»*éšå«*äº†ã€Œä¼˜å…ˆåˆå¹¶æœ€ç»å¸¸å‡ºç°çš„ pairã€è¿™ä»¶äº‹äº†ï¼Œæ³¨æ„ä½“ä¼š**

### BPE ä¾‹å­

åœç•™åœ¨ç®—æ³•ä¸æä¾›ä¾‹å­çš„è¯ï¼Œç»å¸¸è¿˜æ˜¯ä¼šäº‘é‡Œé›¾é‡Œï¼Œæ‰€ä»¥ç°åœ¨æ¥ç»“åˆä¸€ä¸ªä¾‹å­çœ‹ BPE æ˜¯å¦‚ä½•å·¥ä½œçš„

*æ¯”å¦‚è¯­æ–™åº“æ˜¯*
```python
corpus = ["highest", "higher", "lower", "lowest", "cooler", "coolest"]
```

*è¿™é‡Œè·³è¿‡ç»Ÿè®¡è¯é¢‘ï¼Œå› ä¸ºæ¯ä¸€ä¸ªéƒ½æ˜¯ 1ã€‚å…ˆæŠŠæ¯ä¸ªå•è¯å˜æˆä¸€ä¸ªä¸ª utf-8 å­—ç¬¦ç„¶ååŠ ä¸Š `</w>`*
```python
{
    "highest": ["h", "i", "g", "h", "e", "s", "t", "</w>"],
    "higher": ["h", "i", "g", "h", "e", "r", "</w>"],
    "lower": ["l", "o", "w", "e", "r", "</w>"],
    "lowest": ["l", "o", "w", "e", "s", "t", "</w>"],
    "cooler": ["c", "o", "o", "l", "e", "r", "</w>"],
    "collest": ["c", "o", "o", "l", "e", "s", "t", "</w>"],
}
```
*å¯ä»¥çœ‹åˆ° `(e, s)` æ€»å…±å‡ºç°äº† 3 æ¬¡ï¼Œæ˜¯æœ€å¤šæ¬¡çš„ï¼Œç„¶åæ ¹æ®è¿™ä¸ªé‡æ–°åˆ’åˆ†ã€‚æ³¨æ„è¿™é‡Œ `(e, r)` å…¶å®ä¹Ÿæœ‰ä¸€æ ·çš„å‡ºç°é¢‘ç‡ï¼Œæ‰€ä»¥é€‰ `(e, r)` åˆå¹¶ä¹Ÿæ˜¯å¯ä»¥çš„*
```python
{
    "highest": ["h", "i", "g", "h", "es", "t", "</w>"],
    "higher": ["h", "i", "g", "h", "e", "r", "</w>"],
    "lower": ["l", "o", "w", "e", "r", "</w>"],
    "lowest": ["l", "o", "w", "es", "t", "</w>"],
    "cooler": ["c", "o", "o", "l", "e", "r", "</w>"],
    "collest": ["c", "o", "o", "l", "es", "t", "</w>"],
}
```
*æ¥ä¸‹æ¥å‘ç°æœ€å¤šçš„æ˜¯ `(es, t)`ï¼Œæ›´æ–°åˆ’åˆ†*
```python
{
    "highest": ["h", "i", "g", "h", "est", "</w>"],
    "higher": ["h", "i", "g", "h", "e", "r", "</w>"],
    "lower": ["l", "o", "w", "e", "r", "</w>"],
    "lowest": ["l", "o", "w", "est", "</w>"],
    "cooler": ["c", "o", "o", "l", "e", "r", "</w>"],
    "collest": ["c", "o", "o", "l", "est", "</w>"],
}
```
*æ¥ä¸‹æ¥å‘ç°æœ€å¤šçš„æ˜¯ `(est, </w>)`ï¼Œæ›´æ–°åˆ’åˆ†*
```python
{
    "highest": ["h", "i", "g", "h", "est</w>"],
    "higher": ["h", "i", "g", "h", "e", "r", "</w>"],
    "lower": ["l", "o", "w", "e", "r", "</w>"],
    "lowest": ["l", "o", "w", "est</w>"],
    "cooler": ["c", "o", "o", "l", "e", "r", "</w>"],
    "collest": ["c", "o", "o", "l", "est</w>"],
}
```
*æ¥ä¸‹æ¥å‘ç°æœ€å¤šçš„æ˜¯ `(e, r)`ï¼Œæ›´æ–°åˆ’åˆ†*
```python
{
    "highest": ["h", "i", "g", "h", "est</w>"],
    "higher": ["h", "i", "g", "h", "er", "</w>"],
    "lower": ["l", "o", "w", "er", "</w>"],
    "lowest": ["l", "o", "w", "est</w>"],
    "cooler": ["c", "o", "o", "l", "er", "</w>"],
    "collest": ["c", "o", "o", "l", "est</w>"],
}
```
*æ¥ä¸‹æ¥å‘ç°æœ€å¤šçš„æ˜¯ `(er, </w>)`ï¼Œæ›´æ–°åˆ’åˆ†*
```python
{
    "highest": ["h", "i", "g", "h", "est</w>"],
    "higher": ["h", "i", "g", "h", "er</w>"],
    "lower": ["l", "o", "w", "er</w>"],
    "lowest": ["l", "o", "w", "est</w>"],
    "cooler": ["c", "o", "o", "l", "er</w>"],
    "collest": ["c", "o", "o", "l", "est</w>"],
}
```
*åé¢è¿˜å¯ä»¥ç»§ç»­è¿­ä»£æ›´æ–°ï¼Œè¿™é‡Œå°±ä¸å±•å¼€äº†ï¼Œç›¸ä¿¡ä¸Šé¢çš„ä¾‹å­å·²ç»å¤Ÿæ¸…æ¥šäº†ã€‚è€Œä¸”åˆ°è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å·²ç»å¾—åˆ°äº† `er, est` è¿™ä¸¤ä¸ªæœ‰æ„ä¹‰çš„åç¼€*

### BPE çš„ Huggingface å®ç°
Huggingface æä¾›çš„ API è¿˜æŒºç®€å•çš„ï¼Œ**å¯ä»¥æ³¨æ„åˆ° `CharBPETokenizer` çš„ `Char`ï¼Œä¹Ÿè¯æ˜äº†å‰é¢æˆ‘è¯´çš„*
```python
from tokenizers import CharBPETokenizer

# Instantiate tokenizer
tokenizer = CharBPETokenizer()

tokenizer.train_from_iterator(
    corpus,
    vocab_size=17,
    min_frequency=2,
)
```

### æ‰‹åŠ¨å®ç° BPE
**æœ€å¥½ç†è§£ä¸€ä¸ªç®—æ³•çš„åŠæ³•æ°¸è¿œéƒ½æ˜¯å°è¯•è‡ªå·±å®ç°ä¸€ä¸ª**ã€‚æˆ‘è¿™é‡ŒæŒ‰ç…§å‰é¢æè¿°çš„ç®—æ³•æµç¨‹å®ç°äº†ä¸€ä¸ª `BPE` ç±»ï¼Œå¦‚æœåˆå§‹åŒ–çš„æ—¶å€™è®¾ç½® `debug=False` å°±å¯ä»¥çœ‹åˆ°æ•´ä¸ª BPE æ˜¯å¦‚ä½•æ›´æ–°çš„

*é¦–å…ˆæ¥çœ‹æ„é€ å‡½æ•°å’Œç”¨æ¥è®­ç»ƒçš„ `train` æ–¹æ³•*
```python
from collections import defaultdict, Counter
from pprint import pprint


class BPE:
    def __init__(
        self,
        corpus: list[str],
        vocab_size: int,
        max_iter: int | None = None,
        debug: bool = False,
    ):
        self.corpus = corpus
        self.vocab_size = vocab_size
        self.vocab = []
        self.word_freq = Counter()
        self.splits = {}  # e.g. highest: [high, est</w>]
        self.merges = {}  # e.g. [high, est</w>]: highest
        self.max_iter = max_iter
        self.debug = debug

    def train(self):
        """Train a BPE Tokenizer"""
        # count the word frequency
        for document in self.corpus:
            # split each document in corpus by whitespace
            words = document.split()
            self.word_freq += Counter(words)

        # initialize the self.splits
        for word in self.word_freq:
            self.splits[word] = list(word) + ["</w>"]

        if self.debug:
            print(f"Init splits: {self.splits}")

        alphabet = set()
        for word in self.word_freq:
            alphabet |= set(list(word))
        alphabet.add("</w>")

        self.vocab = list(alphabet)
        self.vocab.sort()

        cnt = 0
        while len(self.vocab) < self.vocab_size:
            if self.max_iter and cnt >= self.max_iter:
                break

            # find the most frequent pair
            pair_freq = self.get_pairs_freq()

            if len(pair_freq) == 0:
                print("No pair available")
                break

            pair = max(pair_freq, key=pair_freq.get)

            self.update_splits(pair[0], pair[1])

            if self.debug:
                print(f"Updated splits: {self.splits}")

            self.merges[pair] = pair[0] + pair[1]

            self.vocab.append(pair[0] + pair[1])

            if self.debug:
                print(
                    f"Most frequent pair({max(pair_freq.values())} times) "
                    f"is : {pair[0]}, {pair[1]}. Vocab size: {len(self.vocab)}"
                )

            cnt += 1

```
*æµç¨‹è¿˜æ˜¯æŒºæ¸…æ™°çš„ï¼Œæ ¸å¿ƒçš„å‡ ä¸ªå‡½æ•°å®ç°å¦‚ä¸‹*
```python
    ...
    def update_splits(self, lhs: str, rhs: str):
        """If we see lhs and rhs appear consecutively, we merge them"""
        for word, word_split in self.splits.items():
            new_split = []
            cursor = 0
            while cursor < len(word_split):
                if (
                    word_split[cursor] == lhs
                    and cursor + 1 < len(word_split)
                    and word_split[cursor + 1] == rhs
                ):
                    new_split.append(lhs + rhs)
                    cursor += 2
                else:
                    new_split.append(word_split[cursor])
                    cursor += 1
            self.splits[word] = new_split

            # if word_split != new_split:
            #     print(f"old: {word_split}")
            #     print(f"new: {new_split}")

    def get_pairs_freq(self) -> dict:
        """Compute the pair frequency"""
        pairs_freq = defaultdict(int)
        for word, freq in self.word_freq.items():
            split = self.splits[word]
            for i in range(len(split)):
                if i + 1 < len(split):
                    pairs_freq[(split[i], split[i + 1])] += freq

        return pairs_freq
```
*æœ€åæˆ‘ä»¬å°±å¯ä»¥å†™ä¸€ä¸ª `tokenize` å‡½æ•°*
```python
    ...
    def tokenize(self, s: str) -> list[str]:
        splits = [list(t) + ["</w>"] for t in s.split()]

        for lhs, rhs in self.merges:
            for idx, split in enumerate(splits):
                new_split = []
                cursor = 0
                while cursor < len(split):
                    if (
                        cursor + 1 < len(split)
                        and split[cursor] == lhs
                        and split[cursor + 1] == rhs
                    ):
                        new_split.append(lhs + rhs)
                        cursor += 2
                    else:
                        new_split.append(split[cursor])
                        cursor += 1
                assert "".join(new_split) == "".join(split)
                splits[idx] = new_split

        return sum(splits, [])
```

*å°è¯•ç”¨è‡ªå·±å†™çš„ BPE å¯¹åˆšæ‰çš„ `corpus` è¿›è¡Œåˆ†è¯*
```python
bpe = BPE(corpus, vocab_size=17, debug=False)
bpe.train()
bpe.tokenize(" ". join(corpus))
```

è¾“å‡ºæ˜¯ï¼š
```python
['h', 'i', 'g', 'h', 'est</w>',
 'h', 'i', 'g', 'h', 'er</w>',
 'l', 'o', 'w', 'er</w>',
 'l', 'o', 'w', 'est</w>',
 'c', 'o', 'o', 'l', 'er</w>',
 'c', 'o', 'o', 'l', 'est</w>']
```

ğŸ¤”ï¸ è¯´æ˜ä»£ç å†™å¯¹äº†ï¼Œè€Œä¸”å¤šäºäº† `</w>`ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆæ¸…æ¥šçœ‹åˆ°å•è¯ä¹‹é—´çš„è¾¹ç•Œï¼Œä¹Ÿå¯ä»¥è¿˜åŸå‡ºæœ¬æ¥çš„è¾“å…¥


### æ€»ç»“

BPE ç®—æ³•ç®€å•è€Œä¸”å¾ˆå¥½ç”¨ï¼Œ**ä½†æ˜¯å½“æ·±å…¥åˆ°å®ç°çš„æ—¶å€™ï¼Œä½ å‘ç°ä¼šæœ‰ä¸å°‘ç»†èŠ‚é—®é¢˜ï¼Œä½†æ­£æ˜¯å› ä¸ºæ¥è§¦åˆ°è¿™äº›ç»†èŠ‚æ‰ä½¿å¾—å¯¹ BPE çš„ç†è§£æ›´åŠ æ·±åˆ»**

è¿™é‡Œå¯ä»¥è®¨è®ºä¸€ä¸‹ BPE çš„å±€é™æ€§ï¼Œé‚£å°±æ˜¯ä½ ä¼šå‘ç°**æŠŠæ–‡æ¡£å˜æˆä¸€ä¸ªä¸ªå•è¯æˆ‘ä»¬è¿™é‡Œç”¨çš„æ˜¯ç©ºæ ¼åˆ’åˆ†**ï¼Œä½†æ˜¯åƒä¸­æ–‡çš„è¯ï¼Œç©ºæ ¼å¹¶ä¸æ˜¯å•è¯ä¹‹é—´çš„è¾¹ç•Œï¼Œè¿™å°±ä¼šè®©äº‹æƒ…å˜å¾—æ¯”è¾ƒæ£˜æ‰‹äº†èµ·æ¥

