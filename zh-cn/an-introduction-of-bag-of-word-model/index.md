# è¯è¢‹æ¨¡å‹ç®€ä»‹


## ä»€ä¹ˆæ˜¯è¯è¢‹æ¨¡å‹

åœ¨ NLP ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å°†æ–‡æ¡£ï¼ˆdocumentï¼‰è¡¨ç¤ºä¸ºå‘é‡ï¼Œè¿™æ˜¯å› ä¸ºæœºå™¨å­¦ä¹ åªèƒ½å¤Ÿå¤„ç†æ•°å­—ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬è¦æ‰¾åˆ°ä¸‹é¢è¿™ä¹ˆä¸€ä¸ª*ç¥å¥‡*çš„å‡½æ•°ï¼š

$$
f(\text{document}) = vector
$$

ä»Šå¤©è¦è®¨è®ºçš„æ˜¯è¯è¢‹æ¨¡å‹ï¼ˆbag-of-word, BoWï¼‰ï¼Œè¯è¢‹æ¨¡å‹å¯ä»¥è®©æˆ‘ä»¬æŠŠè¾“å…¥çš„æ–‡æ¡£è½¬å˜æˆä¸€ä¸ªå‘é‡è¡¨ç¤º

> ğŸ’¡ å°½ç®¡è¯è¢‹æ¨¡å‹åœ¨ 2023 å¹´å·²ç»è¿‡æ—¶äº†ï¼Œæˆ‘ä»ç„¶é¼“åŠ±ä½ å­¦ä¹ è¯è¢‹æ¨¡å‹ï¼Œå¹¶ä¸”æ€è€ƒä¸‹é¢å‡ ä¸ª**é‡è¦é—®é¢˜**ï¼š
- Motivation æ˜¯ä»€ä¹ˆï¼Ÿ
- ä¼˜ç¼ºç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ
- å¦‚ä½•æŠŠå®ƒå˜å¾—æ›´å¥½ï¼Ÿ

### è¯è¢‹æ¨¡å‹çš„ motivation å’Œç›´è§‚ç†è§£
åœ¨äº†è§£è¯è¢‹æ¨¡å‹ç»†èŠ‚ä¹‹å‰ï¼Œæˆ‘æƒ³å…ˆç»™ä½ ä¸€ä¸ªè¯è¢‹æ¨¡å‹å¯èƒ½æ˜¯æœ‰ç”¨çš„**ç›´è§‚ç†è§£** â€”â€” **ç›¸ä¼¼çš„æ–‡æ¡£ç”¨çš„è¯*ä¹Ÿè®¸*æ˜¯å·®ä¸å¤šçš„**

ä½ å¯èƒ½æŒåå¯¹æ„è§ï¼Œå¹¶ä¸”å¯ä»¥ç»™å‡ºè®¸å¤šåä¾‹ã€‚æˆ‘æ‰¿è®¤ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦æ›´åŠ å¼ºå¤§çš„æ¨¡å‹ :)

### è¯è¢‹æ¨¡å‹ç»†èŠ‚

æ„é€ ä¸€ä¸ªè¯è¢‹æ¨¡å‹åªéœ€è¦åšä¸¤ä»¶äº‹æƒ…
1. åˆ›å»ºè¯æ±‡è¡¨ï¼ˆVocabï¼‰ï¼Œæ¯ä¸ªå•è¯éƒ½åœ¨è¯æ±‡è¡¨é‡Œé¢æœ‰ä¸€ä¸ªç‹¬ç‰¹çš„ IDï¼ˆä¸€èˆ¬ä» `0` å¼€å§‹ï¼‰ã€‚**æ³¨æ„è¯è¢‹æ¨¡å‹è¾“å‡ºçš„å‘é‡çš„é•¿åº¦ç­‰äºè¯æ±‡è¡¨çš„å¤§å°**
2. éå†è¯­æ–™åº“ä¸­çš„æ¯ä¸ªæ–‡æ¡£ï¼ŒæŠŠè¿™ä¸ªæ–‡æ¡£æ–°å‡ºç°çš„å•è¯æ·»åŠ åˆ°è¯æ±‡è¡¨é‡Œé¢

åœ¨æ„é€ å¥½è¯è¢‹æ¨¡å‹ä¹‹åï¼Œå°±å¯ä»¥æŠŠä»»ä½•æ–‡æ¡£éƒ½è½¬å˜æˆå‘é‡è¡¨ç¤ºäº†ã€‚æ–¹æ³•ä¹Ÿå¾ˆç®€å•ï¼Œåªè¦ç»Ÿè®¡æ–‡æ¡£ä¸­æ¯ä¸ªå•è¯å‡ºç°çš„æ¬¡æ•°ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œå¯¹äºæ–‡æ¡£ä¸­ä¸åœ¨è¯æ±‡è¡¨é‡Œé¢çš„å•è¯ï¼Œ**ä¸€èˆ¬ç›´æ¥å°±å¿½ç•¥ä¸è®¡**

è®©æˆ‘ä»¬æ¥ç»“åˆä¸€ä¸ªç®€å•ä¾‹å­ç†è§£[^1]


```python
toy_corpus = [
    'This is the first document.',
    'This is the second second document.',
    'And the third one.',
    'Is this the first document?',
]
```

åˆ é™¤æ ‡ç‚¹ç¬¦å·ï¼Œç„¶åç”¨ç©ºæ ¼åˆ†è¯ï¼ŒåŒæ—¶æŠŠæ‰€æœ‰çš„å•è¯å˜æˆå°å†™ï¼Œé¢„å¤„ç†ä¹‹åï¼Œæˆ‘ä»¬å°±å¯ä»¥å¾—åˆ°


```python
tokenized_toy_corpus = [
    ['this', 'is', 'the', 'first', 'document'],
    ['this', 'is', 'the', 'second', 'second', 'document'],
    ['and', 'the', 'third', 'one'],
    ['is', 'this', 'the', 'first', 'document']
]
```

ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬è¿™é‡Œè€ƒè™‘æŠŠæ‰€æœ‰æ–‡æ¡£çš„æ‰€æœ‰å•è¯éƒ½åŒ…æ‹¬åœ¨è¯æ±‡è¡¨é‡Œé¢


```python
flatten_list_as_set = set(sum(tokenized_toy_corpus, start=[]))
print(f"the toy vocab size: {len(flatten_list_as_set)}")
```

    the toy vocab size: 9


> ğŸ’¡ ä¸€ä¸ªç”¨ `sum` å‡½æ•°æ‘Šå¹³ list çš„å°æŠ€å·§ :D

ç„¶åï¼Œè®©æˆ‘ä»¬ç»™è¯æ±‡è¡¨é‡Œé¢çš„æ¯ä¸ªå•è¯ä¸€ä¸ªç‹¬ç‰¹çš„ ID


```python
toy_token2id = {}
for token in sorted(flatten_list_as_set):
    toy_token2id[token] = len(toy_token2id)

print(toy_token2id)
```

    {
        'and': 0, 'document': 1, 'first': 2, 
        'is': 3, 'one': 4, 'second': 5,
        'the': 6, 'third': 7, 'this': 8
    }

å¯ä»¥çœ‹åˆ°ï¼Œè¯æ±‡è¡¨çš„å¤§å°æ˜¯ `9`ï¼Œé‚£ä¹ˆæˆ‘ä»¬ä¹Ÿå°±çŸ¥é“ï¼Œæ¯ä¸ªæ–‡æ¡£å¯ä»¥ç”¨ä¸€ä¸ªé•¿åº¦ä¸º `9` çš„å‘é‡è¡¨ç¤º

è®©æˆ‘ä»¬æ‰‹åŠ¨è®¡ç®—ä¸€ä¸‹æ¯ä¸ªæ–‡æ¡£çš„è¯è¢‹æ¨¡å‹å‘é‡æ¥æ£€æŸ¥æˆ‘ä»¬æ˜¯å¦ç†è§£æ­£ç¡®


```python
BoW_matrix = []
for document in tokenized_toy_corpus:
    temp = [0] * 9
    for token in document:
        temp[toy_token2id[token]] += 1
    BoW_matrix.append(temp)
print(BoW_matrix)
```

    [
        [0, 1, 1, 1, 0, 0, 1, 0, 1],
        [0, 1, 0, 1, 0, 2, 1, 0, 1],
        [1, 0, 0, 0, 1, 0, 1, 1, 0],
        [0, 1, 1, 1, 0, 0, 1, 0, 1]
    ]

ç›´æ¥çœ‹æ•°å­—çš„è¯ä¸æ˜¯å¾ˆç›´è§‚ï¼Œå¯ä»¥å¤šå¢åŠ ä¸€è¡Œä¸€åˆ—ï¼Œå°±èƒ½æŠŠå•è¯å’Œè¯é¢‘å¯¹åº”ä¸Šäº†ã€‚å¦‚æœä½ æ£€æŸ¥ç­”æ¡ˆ[^1]çš„è¯ï¼Œä½ ä¼šå‘ç°å’Œæˆ‘ä»¬ç®—å‡ºæ¥çš„ä¸€æ¨¡ä¸€æ ·

|           | and | document | first | is  | one | second | the | third | this |
| --------- | --- | -------- | ----- | --- | --- | ------ | --- | ----- | ---- |
| document1 | 0   | 1        | 1     | 1   | 0   | 0      | 1   | 0     | 1    |
| document2 | 0   | 1        | 0     | 1   | 0   | 2      | 1   | 0     | 1    |
| document3 | 1   | 0        | 0     | 0   | 1   | 0      | 1   | 1     | 0    |
| document4 | 0   | 1        | 1     | 1   | 0   | 0      | 1   | 0     | 1    |

å¦‚ä½•è§£è¯»è¿™ä¸ªè¡¨æ ¼ï¼Ÿ

**æ¯ä¸€è¡Œå°±æ˜¯å¯¹åº”æ–‡æ¡£çš„è¯è¢‹æ¨¡å‹è¾“å‡ºçš„å‘é‡è¡¨ç¤º**ï¼Œä»¥ document2 ä¸ºä¾‹ï¼Œå®ƒåŒ…å«äº†ä¸‹é¢è¿™äº›å•è¯
- `document` * 1
- `is` * 1
- `second` * 2
- `the` * 1
- `this` * 1

ä¹‹å‰æåˆ°ï¼Œåˆ†è¯ä¹‹åçš„ document2 æ˜¯ `['this', 'is', 'the', 'second', 'second', 'document']`ï¼Œæ˜¾ç„¶å’Œè¿™ä¸ªå‘é‡è¡¨ç¤ºæ˜¯å¯¹åº”çš„

ç°åœ¨ä½ å°±çŸ¥é“å¦‚ä½•è§£è¯»è¿™ä¸ªäº†çŸ©é˜µäº† :D

> ğŸ§ ä½ ä¹Ÿè®¸æ³¨æ„åˆ°äº†ï¼Œè¿™ä¸ªçŸ©é˜µä¸­æœ‰å¾ˆå¤š `0`ã€‚BoW çš„çŸ©é˜µç¡®å®æ˜¯è¿™æ ·çš„ï¼Œæ˜¯ä¸€ä¸ªç¨€ç–çŸ©é˜µï¼Œè¿™ä¹Ÿæ˜¯å®ƒçš„ç¼ºç‚¹ä¹‹ä¸€


```python
from sklearn.metrics.pairwise import cosine_similarity
```

æˆ‘ä»¬å¯ä»¥ç”¨å‘é‡å†…ç§¯è®¡ç®— 2 ä¸ªå‘é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦

ä¹‹å‰çš„ `tokenized_toy_corpus` å¦‚ä¸‹
```python
tokenized_toy_corpus = [
    ['this', 'is', 'the', 'first', 'document'],
    ['this', 'is', 'the', 'second', 'second', 'document'],
    ['and', 'the', 'third', 'one'],
    ['is', 'this', 'the', 'first', 'document']
]
```

ç°åœ¨ï¼Œå‡è®¾æŸ¥è¯¢ï¼ˆqueryï¼‰æ˜¯æœ€åä¸€ä¸ªæ–‡æ¡£ â€”â€” `['is', 'this', 'the', 'first', 'document']`ï¼Œå‰ 3 ä¸ªæ–‡æ¡£ä¸­å“ªä¸€ä¸ªè·Ÿå®ƒæœ€åƒï¼Ÿ

æˆ‘ä»¬å¯ä»¥ä¸€çœ¼çœ‹å‡ºç­”æ¡ˆæ˜¯ç¬¬ä¸€ä¸ªï¼Œé‚£ä¹ˆæœºå™¨èƒ½å¦ä¹Ÿå‘ç°è¿™ç‚¹ï¼Ÿ


```python
print(
    cosine_similarity([BoW_matrix[3]], [BoW_matrix[0]]),
    cosine_similarity([BoW_matrix[3]], [BoW_matrix[1]]),
    cosine_similarity([BoW_matrix[3]], [BoW_matrix[2]]),
)
```

    [[1.]] [[0.63245553]] [[0.2236068]]


ğŸ¤”ï¸ æœºå™¨ä¹Ÿçœ‹å‡ºæ¥äº†

## æ›´å¤æ‚çš„çœŸå®æ•°æ®é›†

å‰é¢çš„ä¾‹å­å¤ªç®€å•äº†ï¼Œä¸‹é¢æˆ‘ä¼šç”¨ä¸€ä¸ªæ›´å¤æ‚çš„çœŸå®æ•°æ®é›† â€”â€” [CodeSearchNet](https://huggingface.co/datasets/code_search_net)ã€‚å®ƒåŒ…å«äº†è®¸å¤šç¼–ç¨‹è¯­è¨€çš„å‡½æ•°ï¼Œè¿™é‡Œæˆ‘æŒ‘ Python æ¥å±•ç¤º

å½“ç„¶ä½ ä¹Ÿå¯ä»¥é€‰æ‹©ä½ æ„Ÿå…´è¶£çš„ç¼–ç¨‹è¯­è¨€ :)

```python
from datasets import load_dataset
from gensim import corpora

def process_data(partition: str) -> list[str]:
    """
    Get data from the datasets library from huggingface.
    Only keep the `whole_func_string` column

    Arg
    ---
    `partition`: train/validation/test

    Return
    -----
        return a list of python functions
    """
    raw_datasets = load_dataset("code_search_net", "python")
    return raw_datasets[partition]["whole_func_string"]
```

å–å†³äºä½ çš„ç½‘ç»œè¿™å¯èƒ½å¾—èŠ±ä¸Šä¸€ç‚¹æ—¶é—´ï¼Œå‹ç¼©æ–‡ä»¶å¤§å°æ˜¯ 941MB


```python
# use the test dataset to speed up the process
corpus = process_data("test")
```

æ¥çœ‹ä¸€ä¸‹æ•°æ®ä¸­çš„ä¸€ä¸ªä¾‹å­


```python
print(corpus[0])
```

    def get_vid_from_url(url):
            """Extracts video ID from URL.
            """
            return match1(url, r'youtu\.be/([^?/]+)') or \
              match1(url, r'youtube\.com/embed/([^/?]+)') or \
              match1(url, r'youtube\.com/v/([^/?]+)') or \
              match1(url, r'youtube\.com/watch/([^/?]+)') or \
              parse_query_param(url, 'v') or \
              parse_query_param(parse_query_param(url, 'u'), 'v')


åœ¨å‰é¢çš„è‹±æ–‡åˆ†è¯ä¸­ï¼Œæˆ‘ä»¬åˆ é™¤äº†æ ‡ç‚¹ç¬¦å·ï¼Œç”¨ç©ºæ ¼åˆ†è¯ã€‚ä»£ç çš„åˆ†è¯åˆ™æ¯”è¾ƒä¸ä¸€æ ·ï¼Œç¼–ç¨‹è¯­è¨€æœ‰è‡ªå·±å¯¹åº”çš„è¯­æ³•ï¼ˆä¸Šä¸‹æ–‡æ— å…³æ–‡æ³•ï¼‰ï¼Œé‚£ä¹ˆå°±å¯ä»¥ç”¨è¯æ³•åˆ†æå™¨æ‹¿åˆ°ä¸€ä¸ªä¸ª tokenã€‚æˆ‘è¿™é‡Œç”¨ Python è‡ªå¸¦çš„ `ast` å’Œ `tokenize` ç®€å•å®ç°äº†ä¸€ä¸‹

> ä¸‹é¢çš„å‡½æ•°å¦‚æœçœ‹ä¸æ‡‚ä¹Ÿæ²¡å…³ç³»ï¼Œå¯ä»¥ç›´æ¥è·³è¿‡ã€‚ç”¨è¯æ³•åˆ†æå™¨åˆ†è¯åªæ˜¯ä¸ºäº†å¾—åˆ°æ›´ç²¾ç¡®çš„åˆ†è¯ç»“æœ :)


```python
import ast
from io import BytesIO
import tokenize

def get_token_stream(code: str) -> list[str]:
    """
    Tokenize the source code and return a token stream

    Note that the following token type will be removed:
    - COMMENT
    - NEWLINE
    - NL
    - INDENT
    - DEDENT
    - ENCODING
    - STRING
    """
    # see https://docs.python.org/3/library/token.html
    useless_token_type = {
        tokenize.COMMENT,
        tokenize.NEWLINE,
        tokenize.NL,  # non-terminating newline
        tokenize.INDENT,
        tokenize.DEDENT,
        tokenize.ENCODING,
        tokenize.STRING,
    }
    parse_tree = ast.parse(code)
    origin_tokens = tokenize.tokenize(BytesIO(code.encode("utf-8")).readline)
    token_as_strlist = [
        token.string
        for token in origin_tokens
        if token.type not in useless_token_type
    ]

    return token_as_strlist
```

æ³¨æ„ä¸¤ä»¶äº‹æƒ…ï¼š
- æˆ‘åˆ é™¤äº†**æ‰€æœ‰**çš„å­—ç¬¦ä¸²ï¼ŒåŒ…æ‹¬æ–‡æ¡£ã€f-stringã€æ™®é€šå­—ç¬¦ä¸²å’Œæ³¨é‡Šç­‰
- æˆ‘**æ²¡æœ‰**æŠŠå˜é‡åæˆ–è€…å‡½æ•°åæ ¹æ® `camelCase` æˆ–è€… `snake_case` è¿™ä¸¤ç§å‘½åæƒ¯ä¾‹è¿›ä¸€æ­¥åˆ†è¯

é¦–å…ˆï¼Œå…ˆç”¨ `get_token_stream` æŠŠæ•°æ®ä¸­çš„ Python ä»£ç éƒ½åˆ†è¯ä¸€ä¸‹ã€‚æ³¨æ„æ•°æ®ä¸­åŒ…å«äº† Python2 å’Œ Python3ï¼Œè¿™é‡Œç›´æ¥ä¸¢å¼ƒäº† Python2 çš„ä»£ç 


```python
from tqdm.auto import tqdm

py2_cnt, py3_cnt = 0, 0
codes = []
for code in tqdm(corpus):
    try:
        codes.append(get_token_stream(code))
        py3_cnt += 1
    except SyntaxError:
        py2_cnt += 1
print(f"Python2: {py2_cnt}, Python3: {py3_cnt}")
```

è®©æˆ‘ä»¬éªŒè¯ä¸€ä¸‹ `get_token_stream` æ˜¯å¦åˆ†è¯åˆ†å¯¹äº†


```python
print(codes[0])
```

    [
        'def', 'get_vid_from_url', '(', 'url', ')',
        ':', 'return', 'match1', '(', 'url', ',', ')',
        'or', 'match1', '(', 'url', ',', ')', 'or',
        'match1', '(', 'url', ',', ')', 'or', 'match1',
        '(', 'url', ',', ')', 'or', 'parse_query_param',
        '(', 'url', ',', ')', 'or', 'parse_query_param',
        '(', 'parse_query_param', '(', 'url', ',', ')',
        ',', ')', ''
    ]


ç°åœ¨ï¼Œå¯ä»¥åˆ©ç”¨ [Gensim](https://radimrehurek.com/gensim/) æä¾›çš„ API æ¥åˆ›å»ºè¯æ±‡è¡¨


```python
from gensim import corpora

dictionary = corpora.Dictionary(codes)

print(dictionary)
```

    Dictionary<77242 unique tokens: ['', '(', ')', ',', ':']...>


å¯ä»¥çœ‹åˆ°ï¼Œè¯æ±‡è¡¨å¤ªå¤§äº†ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹èƒ½å¦ä¼˜åŒ–ä¸€ä¸‹

ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬ä¸ä¼šå…³å¿ƒ**åªå‡ºç°ä¸€æ¬¡**çš„ tokenï¼Œå› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠä»–ä»¬åˆ é™¤æ‰


```python
once_ids = [
    token_id
    for token_id, doc_freq in dictionary.dfs.items()
    if doc_freq == 1
]

dictionary.filter_tokens(once_ids)
dictionary.compactify()

print(dictionary)
```

    Dictionary<31933 unique tokens: ['', '(', ')', ',', ':']...>


ç°åœ¨åªå‰©ä¸‹ `31933` tokens äº†ï¼Œè¿™å·²ç»å¥½å¤šäº†ï¼Œæˆ‘çŒœæµ‹ token è¿™ä¹ˆå¤šçš„åŸå› æ˜¯æœ‰å¾ˆå¤šå˜é‡/å‡½æ•°åéƒ½ä¸ä¸€æ · ğŸ§

`Dictionary` æä¾›äº† `most_common` æ–¹æ³•å¯ä»¥æ‰¾åˆ°è¯é¢‘æœ€å¤šçš„ tokenï¼Œè®©æˆ‘ä»¬çœ‹ä¸‹èƒ½å¦å‘ç°ä¸€äº›æœ‰æ„æ€çš„äº‹æƒ…

```python
dictionary.most_common(25)
```




    [
        ('.', 202834), ('(', 199868), (')', 199868), (',', 162853),
        ('=', 142808), (':', 110829), ('self', 71699), ('[', 55736),
        (']', 55736), ('if', 40272), ('return', 24021), ('def', 23557),
        ('', 21948), ('None', 19797), ('in', 19437), ('for', 13509),
        ('1', 13345), ('0', 13213), ('not', 11826), ('else', 10634),
        ('+', 10617), ('==', 9323), ('name', 8290), ('is', 7601), ('-', 7544)
    ]


ğŸ§ å‘ç°äº†ä¸€ä¸ªæœ‰æ„æ€çš„ç°è±¡ï¼Œ`(` å’Œ `)` çš„è¯é¢‘ä¸€æ ·ï¼Œ`[` å’Œ `]` ä¹Ÿæ˜¯çš„ï¼Œå› ä¸ºä»£ç çš„è¯­æ³•å°±æ˜¯è¿™ä¹ˆè§„å®šçš„

ç„¶åç”¨ `doc2bow` API å°±å¯ä»¥å¾—åˆ°æ¯ä¸ªæ–‡æ¡£ï¼ˆä»£ç ï¼‰çš„è¯è¢‹æ¨¡å‹å‘é‡


```python
BoW_matrix_for_code = [dictionary.doc2bow(d) for d in codes]
print(BoW_matrix_for_code[0])
```

    [
        (0, 1), (1, 8), (2, 8), (3, 7),
        (4, 1), (5, 1), (6, 1), (7, 4),
        (8, 5), (9, 3), (10, 1), (11, 7)
    ]

`doc2bow` çš„è¿”å›å€¼æ˜¯ä¸€ä¸ª tuple listï¼Œæ¯ä¸ª tuple çš„å«ä¹‰æ˜¯ `(token çš„ IDï¼Œè¿™ä¸ª token çš„è¯é¢‘)`ï¼Œæ³¨æ„ç»Ÿè®¡è¯é¢‘æ˜¯åœ¨è¿™ä¸ªæ–‡æ¡£é‡Œé¢è¿›è¡Œçš„

Gensim ç”¨çš„è¿™ä¸ªæ ¼å¼æ˜¯åˆç†çš„ï¼Œå› ä¸º BoW ç®—å‡ºæ¥çš„çŸ©é˜µæ˜¯ç¨€ç–çš„ï¼Œè€Œä¸”è¿™é‡Œçš„è¯æ±‡è¡¨å¤§å°æœ‰ `31933`ï¼Œåªå±•ç¤ºé 0 çš„ä½ç½®å°±æ¸…æ¥šå¾ˆå¤š

è®©æˆ‘ä»¬æŠŠæ¯ä¸ª token id æ›¿æ¢ä¸ºå¯¹åº”çš„å­—ç¬¦ä¸²è¡¨ç¤º


```python
[
    (dictionary[token_id], cnt)
    for token_id, cnt in BoW_matrix_for_code[0]
]
```




    [
        ('', 1), ('(', 8), (')', 8), (',', 7),
        (':', 1), ('def', 1), ('get_vid_from_url', 1), ('match1', 4),
        ('or', 5), ('parse_query_param', 3), ('return', 1), ('url', 7)
    ]



ä¸‹ä¸€ä»¶æƒ³è¦åšçš„äº‹æƒ…æ˜¯ï¼š**èƒ½å¦è®©è¯è¢‹æ¨¡å‹æ ¹æ®æˆ‘ä»¬æä¾›çš„ Python ä»£ç æ‰¾åˆ°ç›¸ä¼¼çš„ Python ä»£ç **ï¼Ÿ


```python
from gensim.similarities import Similarity

indexer = Similarity(
    output_prefix=None,
    corpus=BoW_vectors,
    num_features=len(dictionary),
    num_best=3,                  # let's see Top-3 result
)
```

éšä¾¿å†™ä¸€ä¸ª Python å‡½æ•°æ¥æ£€æŸ¥è¯è¢‹æ¨¡å‹æ˜¯å¦è¿”å›äº†ç›¸ä¼¼çš„ä»£ç 


```python
query = """def foo(x):
    if x > 5:
        if x > 10:
            return x + 1
        else:
            return x - 1
    else:
        if x < 0:
            return x + 1
        else:
            return x - 1
"""
```


```python
indexer[dictionary.doc2bow(get_token_stream(query))]
```




    [
        (19669, 0.7191814184188843),
        (19805, 0.705620288848877),
        (5958, 0.6945071220397949)
    ]




```python
print(corpus[19669])
```

    def add_sms_spec_to_request(self, req, federation='', loes=None,
                                context=''):
        """
        Update a request with signed metadata statements.
        
        :param req: The request 
        :param federation: Federation Operator ID
        :param loes: List of :py:class:`fedoidc.operator.LessOrEqual` instances
        :param context:
        :return: The updated request
        """
        if federation:  # A specific federation or list of federations
            if isinstance(federation, list):
                req.update(self.gather_metadata_statements(federation,
                                                           context=context))
            else:
                req.update(self.gather_metadata_statements([federation],
                                                           context=context))
        else:  # All federations I belong to
            if loes:
                _fos = list([r.fo for r in loes])
                req.update(self.gather_metadata_statements(_fos,
                                                           context=context))
            else:
                req.update(self.gather_metadata_statements(context=context))

        return req


ä½ å¯èƒ½ä¼šå‘ç°æŸ¥è¯¢å’Œç»“æœä¼¼ä¹*åœ¨æŸç§æ„ä¹‰ä¸Š*åŒ¹é…ã€‚å®ƒä»¬æœ‰ä¸€äº›ç›¸ä¼¼çš„*è¯­æ³•*ä¿¡æ¯ï¼ˆåµŒå¥—çš„ `if-else` ç»“æ„ï¼‰

ç„¶è€Œï¼Œ**åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œè¯è¢‹æ¨¡å‹ç»™å‡ºçš„ç»“æœå¾ˆå·®**ã€‚è¿™æ˜¯åˆç†çš„ï¼Œå› ä¸ºè¯è¢‹æ¨¡å‹å¤ªç®€å•äº†ï¼Œæ— æ³•æ‰¾åˆ°ä»£ç ä¹‹é—´çš„å…³ç³»

## æ€»ç»“
ç°åœ¨ï¼Œæˆ‘ä»¬æ¥æ€»ç»“ä¸€ä¸‹ BoW çš„ä¸€äº›ç¼ºç‚¹ã€‚ä½ å¯èƒ½å·²ç»è‡ªå·±å¼„æ¸…æ¥šäº†å…¶ä¸­ä¸€äº›ï¼š
1. å•è¯ä¹‹é—´çš„é¡ºåºä¿¡æ¯æ²¡æœ‰å¾—åˆ°ä¿ç•™ã€‚*`The cat chased the dog` å’Œ `The dog chased the cat` æ„æ€å®Œå…¨ä¸ä¸€æ ·*
2. æ²¡æœ‰è¯­ä¹‰ä¿¡æ¯ã€‚ *è¯è¢‹æ¨¡å‹å°†æ¯ä¸ªå•è¯è§†ä¸ºä¸€ä¸ªç‹¬ç«‹çš„å®ä½“*
3. è¯è¢‹æ¨¡å‹è¾“å‡ºçš„å‘é‡æ˜¯é«˜ç»´ç¨€ç–å‘é‡ã€‚ *è®¡ç®—é‡å¾ˆå¤§ï¼Œå‘é‡é•¿åº¦å–å†³äºä½ çš„è¯æ±‡è¡¨å¤§å°*
4. æ¯ä¸ªè¯éƒ½æœ‰ç›¸åŒçš„é‡è¦æ€§ã€‚ *ä½†æœ‰äº›è¯å¯èƒ½æä¾›æ›´å¤šä¿¡æ¯*
5. æ— æ³•å¤„ç†ä¸åœ¨è¯æ±‡è¡¨é‡Œé¢çš„å•è¯ã€‚ *å¦‚æœæ–‡æ¡£åŒ…å«å¾ˆå¤šä¸åœ¨è¯æ±‡è¡¨ä¸­çš„å•è¯è¦æ€ä¹ˆåŠ*ï¼Ÿ
6. ...

è¯è¢‹æ¨¡å‹æœ‰å¾ˆå¤šç¼ºç‚¹ï¼Œä½ å¯èƒ½åªä¼šåœ¨æ•™ç¨‹é‡Œé¢çœ‹åˆ°å®ƒã€‚é‰´äºè¿™äº›é™åˆ¶ï¼Œåé¢äººä»¬åˆå¼€å‘äº† Word2Vecã€GloVe å’ŒåŸºäº Transformer çš„æ¶æ„ï¼ˆä¾‹å¦‚ BERTã€GPTï¼‰ç­‰æ›´å…ˆè¿›çš„æ¨¡å‹æ¥å…‹æœè¿™äº›ç¼ºç‚¹

## å‚è€ƒ

[^1]: [CountVectorizer](https://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage)


