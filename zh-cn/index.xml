<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>MartinLwx&#39;s Blog</title>
        <link>https://martinlwx.github.io/zh-cn/</link>
        <description>Welcome to my blog :)</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>&lt;a rel=&#34;license noopener&#34; href=&#34;https://creativecommons.org/licenses/by-nc-nd/4.0/&#34; target=&#34;_blank&#34;&gt;CC BY-NC-ND 4.0&lt;/a&gt;</copyright><lastBuildDate>Tue, 18 Feb 2025 23:13:42 &#43;0800</lastBuildDate>
            <atom:link href="https://martinlwx.github.io/zh-cn/index.xml" rel="self" type="application/rss+xml" />
        <item>
    <title>三地址码(3AC/TAC)是什么</title>
    <link>https://martinlwx.github.io/zh-cn/three-address-code/</link>
    <pubDate>Tue, 18 Feb 2025 23:13:42 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/three-address-code/</guid>
    <description><![CDATA[<h2 id="什么是三地址码" class="headerLink">
    <a href="#%e4%bb%80%e4%b9%88%e6%98%af%e4%b8%89%e5%9c%b0%e5%9d%80%e7%a0%81" class="header-mark" aria-label="Header mark for '什么是三地址码'"></a>什么是三地址码</h2><p>三地址码（Three-Address Code，也简记为 3AC/TAC）是一种程序的中间表示（Intermediate Representation，IR），通常用在编译器、程序分析当中。顾名思义，三地址码的每一条指令<em>最多</em>只有三个“地址”，这里的“地址”包括变量、常量。常见的形式包括下面几种 <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>]]></description>
</item><item>
    <title>GraphRAG 工作流</title>
    <link>https://martinlwx.github.io/zh-cn/the-flow-of-graphrag/</link>
    <pubDate>Tue, 11 Feb 2025 23:40:59 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/the-flow-of-graphrag/</guid>
    <description><![CDATA[<h2 id="motivation" class="headerLink">
    <a href="#motivation" class="header-mark" aria-label="Header mark for 'Motivation'"></a>Motivation</h2><p>当前的 RAG 技术无法回答<em>关于语料库的全局性问题</em>，比如“这个数据集的主题是什么”。这一类问题不是可以通过检索增强技术解决的，因为答案一般不在某一段文本里面，正确答案需要<em>理解整个语料库并给出抽象的总结</em>，作者称这类问题为 query-focused summarization (QFS) 问题<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。普通的 RAG 技术无法很好处理这个问题。</p>]]></description>
</item><item>
    <title>论文阅读: Outrageously Large Neural Networks-The Sparsely-Gated Mixture-of-Experts Layer</title>
    <link>https://martinlwx.github.io/zh-cn/reading-notes-mixture-of-experts/</link>
    <pubDate>Sun, 02 Feb 2025 14:22:57 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/reading-notes-mixture-of-experts/</guid>
    <description><![CDATA[<h2 id="motivations" class="headerLink">
    <a href="#motivations" class="header-mark" aria-label="Header mark for 'Motivations'"></a>Motivations</h2><p>模型能力跟模型参数量有关系，模型参数量越多，数据越多，效果就越好。<em>但训练成本也成倍上升</em>。为了解决这个问题，大家提出了很多种<em>条件计算</em>（Conditional Computations）的方案，顾名思义，某些条件满足的情况下才会计算，这样<em>就可以不增加训练成本的同时增加模型参数量，提升模型效果</em></p>]]></description>
</item><item>
    <title>什么是 Python 装饰器</title>
    <link>https://martinlwx.github.io/zh-cn/understanding-python-decorator/</link>
    <pubDate>Mon, 20 Jan 2025 21:40:43 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/understanding-python-decorator/</guid>
    <description><![CDATA[<h2 id="引言" class="headerLink">
    <a href="#%e5%bc%95%e8%a8%80" class="header-mark" aria-label="Header mark for '引言'"></a>引言</h2><p>如果你能够认识到<em>函数是一等公民（First-class）的话</em>，那么你理解 Python 装饰器应该没有什么困难。函数是一等公民（First-class）就意味着：函数也是值，和其他基本类型（<code>int, str, float, etc</code>）等一样，<em>都可以作为函数的入参和返回值</em></p>]]></description>
</item><item>
    <title>论文阅读: Generalization through Memorization: Nearest Neighbor Language Models</title>
    <link>https://martinlwx.github.io/zh-cn/what-is-knn-lm/</link>
    <pubDate>Mon, 23 Dec 2024 22:01:35 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/what-is-knn-lm/</guid>
    <description><![CDATA[<h2 id="motivation" class="headerLink">
    <a href="#motivation" class="header-mark" aria-label="Header mark for 'Motivation'"></a>Motivation</h2><p>语言模型解决 2 种问题</p>
<ol>
<li>用一个特征向量表示句子前缀</li>
<li>使用该特征向量预测下一个 token</li>
</ol>
<p>本文提出的 $k\texttt{NN-LM}$ 基于这么一个假设：<em>学习特征向量表示比预测下一个 token</em>，因此本文的方法主要基于该假设进行设计</p>]]></description>
</item><item>
    <title>KNN 算法是什么</title>
    <link>https://martinlwx.github.io/zh-cn/what-is-k-nearest-neighbor-algorithms/</link>
    <pubDate>Sun, 15 Dec 2024 15:10:00 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/what-is-k-nearest-neighbor-algorithms/</guid>
    <description><![CDATA[<h2 id="什么是-knn" class="headerLink">
    <a href="#%e4%bb%80%e4%b9%88%e6%98%af-knn" class="header-mark" aria-label="Header mark for '什么是 KNN'"></a>什么是 KNN</h2><div class="details admonition tip open">
    <div class="details-summary admonition-title">
        <span class="icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 352 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M96.06 454.35c.01 6.29 1.87 12.45 5.36 17.69l17.09 25.69a31.99 31.99 0 0 0 26.64 14.28h61.71a31.99 31.99 0 0 0 26.64-14.28l17.09-25.69a31.989 31.989 0 0 0 5.36-17.69l.04-38.35H96.01l.05 38.35zM0 176c0 44.37 16.45 84.85 43.56 115.78 16.52 18.85 42.36 58.23 52.21 91.45.04.26.07.52.11.78h160.24c.04-.26.07-.51.11-.78 9.85-33.22 35.69-72.6 52.21-91.45C335.55 260.85 352 220.37 352 176 352 78.61 272.91-.3 175.45 0 73.44.31 0 82.97 0 176zm176-80c-44.11 0-80 35.89-80 80 0 8.84-7.16 16-16 16s-16-7.16-16-16c0-61.76 50.24-112 112-112 8.84 0 16 7.16 16 16s-7.16 16-16 16z"/></svg></span>Tip<span class="details-icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></span>
    </div>
    <div class="details-content">
        <div class="admonition-content"><p>显然，从定义来看，KNN 算法<em>并不需要训练</em></p>]]></description>
</item><item>
    <title>OCaml 中的 Phantom Type 是什么</title>
    <link>https://martinlwx.github.io/zh-cn/what-is-phantom-type-in-ocaml/</link>
    <pubDate>Sun, 08 Dec 2024 16:33:18 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/what-is-phantom-type-in-ocaml/</guid>
    <description><![CDATA[<h2 id="语法" class="headerLink">
    <a href="#%e8%af%ad%e6%b3%95" class="header-mark" aria-label="Header mark for '语法'"></a>语法</h2><div class="details admonition info open">
    <div class="details-summary admonition-title">
        <span class="icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z"/></svg></span>Info<span class="details-icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></span>
    </div>
    <div class="details-content">
        <div class="admonition-content"><p><code>=</code> 左侧是类型（Type），右侧是值（Value）</p>]]></description>
</item><item>
    <title>论文阅读: In-Context Retrieval-Augmented Language Models</title>
    <link>https://martinlwx.github.io/zh-cn/in-context-ralm-paper-reading/</link>
    <pubDate>Wed, 04 Dec 2024 00:53:25 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/in-context-ralm-paper-reading/</guid>
    <description><![CDATA[<h2 id="the-idea" class="headerLink">
    <a href="#the-idea" class="header-mark" aria-label="Header mark for 'The idea'"></a>The idea</h2><p>In-Context RALM<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> 是用于 Autoregressive LM 上的 RAG 技术。RAG 说白了就是在模型推理的时候有个 Retriever 检索相关的文档，检索到的文档会和本来的输入拼接在一起</p>
<p>在 In-Context Learning 里面，会把一些例子放在用户输入的<em>前面</em>，再给 LLM。因此不难想象 In-Context RALM <em>也类似</em>：In-Context RALM 就是将检索到的<em>最相关</em>的文档直接拼在模型输入的<em>前面</em>，优势是<em>不需要再训练 LLM</em>，我用 mermaid 画了一个图，如下所示</p>]]></description>
</item><item>
    <title>论文阅读: REALM: Retrieval-Augmented Language Model Pre-Training</title>
    <link>https://martinlwx.github.io/zh-cn/rag-realm-paper-reading/</link>
    <pubDate>Sat, 30 Nov 2024 00:01:26 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/rag-realm-paper-reading/</guid>
    <description><![CDATA[<h2 id="引言" class="headerLink">
    <a href="#%e5%bc%95%e8%a8%80" class="header-mark" aria-label="Header mark for '引言'"></a>引言</h2><p>最近打算系统性学习 RAG 技术，开始看起了相关文献，目前的思路是按照 ACL 2023 Tutorial 的 <a href="https://acl2023-retrieval-lm.github.io/slides/3-architecture.pdf" target="_blank" rel="noopener noreferrer">Roadmap</a> 过一遍。本篇是对早期的 RAG 技术的 REALM 的介绍</p>
<div class="details admonition info open">
    <div class="details-summary admonition-title">
        <span class="icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z"/></svg></span>Info<span class="details-icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></span>
    </div>
    <div class="details-content">
        <div class="admonition-content"><p>本文采用的模型是 Masked LM 的 BERT，还不是 LLM。因此本文后续的部分内容需要你<em>对 BERT 有一定的了解</em>，包括 BERT 的预训练过程、BERT 微调等</p>]]></description>
</item><item>
    <title>使用 OCaml 中的 Polymorphic Variant 类型</title>
    <link>https://martinlwx.github.io/zh-cn/use-polymorphic-variant-type-in-ocaml/</link>
    <pubDate>Sun, 24 Nov 2024 20:00:58 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/use-polymorphic-variant-type-in-ocaml/</guid>
    <description><![CDATA[<h2 id="引言" class="headerLink">
    <a href="#%e5%bc%95%e8%a8%80" class="header-mark" aria-label="Header mark for '引言'"></a>引言</h2><p>我已经学习并使用了 OCaml 有段时间了，但是一直搞不清楚 Polymorphic variant 有什么用。最近在看 <a href="https://github.com/ocaml-community/yojson" target="_blank" rel="noopener noreferrer">Yojson</a> 的时候又看到了这种用法，一番搜索之后发现并没有看到关于 Polymorphic variant 的比较好的文章（<a href="https://ocaml.org/manual/5.1/polyvariant.html" target="_blank" rel="noopener noreferrer">官方介绍</a> 在我看来有点难懂），只看到一些相关的回答<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup><sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>。经过仔细学习之后，我决定写一篇文章，希望能对你有所帮助 :)</p>]]></description>
</item></channel>
</rss>
