<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Performance-Engineering - Tag - MartinLwx&#39;s Blog</title>
        <link>https://martinlwx.github.io/en/tags/performance-engineering/</link>
        <description>Performance-Engineering - Tag - MartinLwx&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>martinlwx@163.com (MartinLwx)</managingEditor>
            <webMaster>martinlwx@163.com (MartinLwx)</webMaster><copyright>&lt;a rel=&#34;license noopener&#34; href=&#34;https://creativecommons.org/licenses/by-nc-nd/4.0/&#34; target=&#34;_blank&#34;&gt;CC BY-NC-ND 4.0&lt;/a&gt;</copyright><lastBuildDate>Wed, 18 Jun 2025 23:07:55 &#43;0800</lastBuildDate><atom:link href="https://martinlwx.github.io/en/tags/performance-engineering/" rel="self" type="application/rss+xml" /><item>
    <title>Async &#43; Token Bucket: How to Batch LLM API Calls Efficiently</title>
    <link>https://martinlwx.github.io/en/async-and-token-bucket-algorithm-batch-llm-api-call/</link>
    <pubDate>Wed, 18 Jun 2025 23:07:55 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/async-and-token-bucket-algorithm-batch-llm-api-call/</guid>
    <description><![CDATA[<h2 id="background" class="headerLink">
    <a href="#background" class="header-mark"></a>Background</h2><p>Recently, at work, I&rsquo;ve been working on setting up a LLM evaluation platform. There&rsquo;s one particular scenario: we need to call an LLM API provided by another department to run model evaluations on a test dataset, but this LLM API has a rate limit of a maximum of 2 calls per second (2 RPS). Thus, my task essentially boils down to: <em>How to maximize concurrency to speed up model evaluation while strictly adhering to the API rate limits</em>. In this brief post, I will share my thoughts about approaching this task.</p>]]></description>
</item><item>
    <title>Two different APIs related to Process Pool in Python</title>
    <link>https://martinlwx.github.io/en/data-parallel-with-two-different-api/</link>
    <pubDate>Sun, 30 Mar 2025 21:36:18 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/data-parallel-with-two-different-api/</guid>
    <description><![CDATA[<h2 id="intro" class="headerLink">
    <a href="#intro" class="header-mark"></a>Intro</h2><p>As a machine learning engineer, I work with vast amounts of data, performing tasks such as data cleaning and information extraction. These tasks are typically data-parallel and do not involve race conditions. Such workloads are usually referred to as CPU-intensive tasks. Typically, these tasks can be formulated as <code>map(fn, data)</code>.</p>
<p>Python is limited by its Global Interpreter Lock (GIL). As a result, using multithreading could not improve the performance. Instead, multiprocessing should be used. Usually, you would not manually manage all the launched processes but would instead use a process pool. Today, I&rsquo;d like to discuss two different APIs related to multiprocessing: <code>multiprocessing.Pool</code> and <code>concurrent.futures.ProcessPoolExecutor</code>, and provide a simple comparison.</p>]]></description>
</item><item>
    <title>Tail call and Tail-call Optimization (TCO)</title>
    <link>https://martinlwx.github.io/en/tail-call-and-tail-call-optimization/</link>
    <pubDate>Fri, 22 Mar 2024 12:13:53 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/tail-call-and-tail-call-optimization/</guid>
    <description><![CDATA[<h2 id="tail-call--tail-recursion" class="headerLink">
    <a href="#tail-call--tail-recursion" class="header-mark"></a>Tail call &amp; Tail-recursion</h2><p>Assume that function <code>A</code> calls function <code>B</code>. We call function <code>A</code> <em>Caller</em> and function <code>B</code> <em>Callee</em>.</p>
<p>The tail call refers to when the Caller <em>only needs to</em> wait for the return value of the Callee, as <em>everything else</em> has already been completed<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<p>If this is a recursive function call(e.g. the Caller and Callee are the same function), then the function is said to be tail-recursive.</p>
<h3 id="tail-call-optimization-tco" class="headerLink">
    <a href="#tail-call-optimization-tco" class="header-mark"></a>Tail-call Optimization (TCO)</h3><p>According to the mechanism of the function call, a stack frame will be created to store some useful information such as the return address. What if the function call is a tail-call? Could you find any optimization chance?</p>]]></description>
</item></channel>
</rss>
