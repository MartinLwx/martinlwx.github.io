<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Deep-Learning - Tag - MartinLwx&#39;s Blog</title>
        <link>https://martinlwx.github.io/en/tags/deep-learning/</link>
        <description>Deep-Learning - Tag - MartinLwx&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>martinlwx@163.com (MartinLwx)</managingEditor>
            <webMaster>martinlwx@163.com (MartinLwx)</webMaster><copyright>&lt;a rel=&#34;license noopener&#34; href=&#34;https://creativecommons.org/licenses/by-nc-nd/4.0/&#34; target=&#34;_blank&#34;&gt;CC BY-NC-ND 4.0&lt;/a&gt;</copyright><lastBuildDate>Sat, 24 May 2025 16:28:13 &#43;0800</lastBuildDate><atom:link href="https://martinlwx.github.io/en/tags/deep-learning/" rel="self" type="application/rss+xml" /><item>
    <title>Transformer architecture variation: Rotary Position Embedding (RoPE)</title>
    <link>https://martinlwx.github.io/en/rotary-position-embedding/</link>
    <pubDate>Sat, 24 May 2025 16:28:13 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/rotary-position-embedding/</guid>
    <description><![CDATA[<h2 id="a-recap-of-self-attention-mechanism" class="headerLink">
    <a href="#a-recap-of-self-attention-mechanism" class="header-mark"></a>A Recap of Self-attention Mechanism</h2><p>In self-attention, the query ($\mathbf q_m$), key ($\mathbf k_n$), and value ($\mathbf v_n$) vectors are computed as follows:</p>
<p>$$
\begin{aligned}
\mathbf q_m&amp;=f_q(\mathbf x_m,m)\\
\mathbf k_n&amp;=f_k(\mathbf x_n,n)\\
\mathbf v_n&amp;=f_v(\mathbf x_n,n)
\end{aligned}
$$</p>
<p>Here, the $\mathbf x_i$ is the $i$-th token embedding, while $n$ and $m$ denote different positions.</p>
<p>The attention score between position $m$ and $n$ is computed as:</p>
<p>$$
\alpha_{m,n}=\frac{exp(\frac{\mathbf q_m^T\mathbf k_n}{\sqrt d})}{\sum_{j=1}^Nexp(\frac{\mathbf q_m^T\mathbf k_j}{\sqrt d})}
$$</p>]]></description>
</item><item>
    <title>Transformer architecture variation: RMSNorm</title>
    <link>https://martinlwx.github.io/en/rmsnorm-in-a-nutshell/</link>
    <pubDate>Sun, 11 May 2025 14:01:26 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/rmsnorm-in-a-nutshell/</guid>
    <description><![CDATA[<h2 id="intro" class="headerLink">
    <a href="#intro" class="header-mark"></a>Intro</h2><p>It&rsquo;s been 8 years since the famous transformer architecture was first proposed. You might have noticed that some modifications to the original design - for instance, most large language models (LLMs) now use RMSNorm<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> instead of LayerNorm. Today I will briefly introduce RMSNorm, but first, let&rsquo;s recap LayerNorm.</p>
<h2 id="layernorm-recap" class="headerLink">
    <a href="#layernorm-recap" class="header-mark"></a>LayerNorm Recap</h2><p>$$
\mathbf y=\frac{\mathbf x-E[\mathbf x]}{\sqrt{Var(\mathbf x)+\epsilon}}*\gamma+\beta
$$</p>
<p>The equation above shows how LayerNorm works. If we ignore the scaling factors ($\gamma, \beta$), LayerNorm&rsquo;s behavior becomes intuitive: <em>it transforms each input $\mathbf x$ into a feature vector with zero mean and unit standard deviation .</em></p>]]></description>
</item><item>
    <title>One for all: the torch.einsum API</title>
    <link>https://martinlwx.github.io/en/the-magic-torch-einsum-api/</link>
    <pubDate>Mon, 14 Apr 2025 20:44:01 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/the-magic-torch-einsum-api/</guid>
    <description><![CDATA[<h2 id="motivations" class="headerLink">
    <a href="#motivations" class="header-mark"></a>Motivations</h2><p>In PyTorch, <em>multiple APIs</em> exist for matrix multiplication operations. However, these functions often lead to memorization challenges. Additionally, many of these APIs <em>require explicit dimension manipulation</em> (e.g., permuting, reshaping)</p>
<p>Does there exist a <em>magic</em> API that can cover all the use cases? A potential unified solution is the <code>torch.einsum</code> API.</p>
<h2 id="what-is-torcheinsum-" class="headerLink">
    <a href="#what-is-torcheinsum-" class="header-mark"></a>What is torch.einsum ?</h2><p>The syntax of <code>torch.einsum</code> is</p>
<div class="code-block highlight is-open show-line-numbers  tw-group tw-my-2">
  <div class="
    
    tw-flex 
    tw-flex-row
    tw-flex-1 
    tw-justify-between 
    tw-w-full tw-bg-bgColor-secondary
    ">      
    <button 
      class="
        code-block-button
        tw-mx-2 
        tw-flex
        tw-flex-row
        tw-flex-1"
      aria-hidden="true">
          <div class="group-[.is-open]:tw-rotate-90 tw-transition-[transform] tw-duration-500 tw-ease-in-out print:!tw-hidden tw-w-min tw-h-min tw-my-1 tw-mx-1"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M285.476 272.971L91.132 467.314c-9.373 9.373-24.569 9.373-33.941 0l-22.667-22.667c-9.357-9.357-9.375-24.522-.04-33.901L188.505 256 34.484 101.255c-9.335-9.379-9.317-24.544.04-33.901l22.667-22.667c9.373-9.373 24.569-9.373 33.941 0L285.475 239.03c9.373 9.372 9.373 24.568.001 33.941z"/></svg></div>
          <p class="tw-select-none !tw-my-1">python</p>]]></description>
</item><item>
    <title>Weight Tying in Language Models: A Technique to Parameter efficiency</title>
    <link>https://martinlwx.github.io/en/an-explanation-of-weight-tying/</link>
    <pubDate>Tue, 11 Mar 2025 19:05:30 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/an-explanation-of-weight-tying/</guid>
    <description><![CDATA[<h2 id="intro" class="headerLink">
    <a href="#intro" class="header-mark"></a>Intro</h2><div class="details admonition quote open">
    <div class="details-summary admonition-title">
        <span class="icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M464 32H336c-26.5 0-48 21.5-48 48v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48zm-288 0H48C21.5 32 0 53.5 0 80v128c0 26.5 21.5 48 48 48h80v64c0 35.3-28.7 64-64 64h-8c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24h8c88.4 0 160-71.6 160-160V80c0-26.5-21.5-48-48-48z"/></svg></span>Quote<span class="details-icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></span>
    </div>
    <div class="details-content">
        <div class="admonition-content"><p><em>In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation</em> - <em>Attention is All You Need</em>, Section 3.4. Embeddings and Softmax<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>]]></description>
</item><item>
    <title>What is Multi-Head Attention (MHA)</title>
    <link>https://martinlwx.github.io/en/an-explanation-of-multi-head-attention/</link>
    <pubDate>Tue, 04 Mar 2025 23:41:07 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/an-explanation-of-multi-head-attention/</guid>
    <description><![CDATA[<h2 id="whats-multi-head-attention-mha" class="headerLink">
    <a href="#whats-multi-head-attention-mha" class="header-mark"></a>What&rsquo;s Multi-Head Attention (MHA)</h2><p>In <a href="https://martinlwx.github.io/en/an-explanation-of-self-attention/" rel="">last post</a> I have explained how the self-attention mechanism works. Today let&rsquo;s <em>take a step further</em> and explore multi-head attention (MHA), which is the full version of self-attention as described in the original paper<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. Since I have covered most of the foundation concepts in <a href="https://martinlwx.github.io/en/an-explanation-of-self-attention/" rel="">last post</a>, this post will be short. :)</p>
<p>Previously, we mentioned that the self-attention mechanism has three import matrices.</p>
<p>$$
\mathbf Q,\mathbf K,\mathbf V\in\mathcal{R}^{n\times d}
$$</p>]]></description>
</item><item>
    <title>An Explanation of Self-Attention mechanism in Transformer</title>
    <link>https://martinlwx.github.io/en/an-explanation-of-self-attention/</link>
    <pubDate>Sun, 02 Mar 2025 10:50:00 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/an-explanation-of-self-attention/</guid>
    <description><![CDATA[<div class="details admonition info open">
    <div class="details-summary admonition-title">
        <span class="icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z"/></svg></span>Info<span class="details-icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></span>
    </div>
    <div class="details-content">
        <div class="admonition-content"><p><strong>Further reading</strong>:</p>]]></description>
</item><item>
    <title>Reading Notes: Outrageously Large Neural Networks-The Sparsely-Gated Mixture-of-Experts Layer</title>
    <link>https://martinlwx.github.io/en/reading-notes-mixture-of-experts/</link>
    <pubDate>Sun, 02 Feb 2025 14:22:57 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/reading-notes-mixture-of-experts/</guid>
    <description><![CDATA[<h2 id="motivations" class="headerLink">
    <a href="#motivations" class="header-mark"></a>Motivations</h2><p>The model&rsquo;s performance is related to the model&rsquo;s parameter. The bigger the model is, the more performant it will be. However, the <em>computational cost also increases</em>. To mitigate this problem, various forms of conditional computation have been proposed to increase model performance <em>without a proportional increase in computational costs</em><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<p>Today I would like to share the Sparsely-Gated Mixture-of-Experts Layer (MoE) as proposed in this paper.<sup id="fnref1:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<figure><img src="/img/moe.png" width="resize">
</figure>

<h2 id="moe-architecture" class="headerLink">
    <a href="#moe-architecture" class="header-mark"></a>MoE architecture</h2><p>There are $n$ experts in the MoE layer (denoted as $E_1, E_2, &hellip;, E_n$) and they are controlled by a gating network $G$. The output of the gating network $G$ is a vector with length $n$.</p>]]></description>
</item><item>
    <title>Reading Notes: Generalization through Memorization: Nearest Neighbor Language Models</title>
    <link>https://martinlwx.github.io/en/what-is-knn-lm/</link>
    <pubDate>Mon, 23 Dec 2024 22:01:35 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/what-is-knn-lm/</guid>
    <description><![CDATA[<h2 id="motivation" class="headerLink">
    <a href="#motivation" class="header-mark"></a>Motivation</h2><p>A language solves 2 subproblems.</p>
<ol>
<li>Mapping sentence prefixes to fixed-size representation.</li>
<li>Using these representations to predict the next token in the context.</li>
</ol>
<p>The $k\texttt{NN-LM}$ proposed in this hypothesis that <em>representation learning problem may be easier than the prediction problem</em></p>
<h2 id="knn-lm" class="headerLink">
    <a href="#knn-lm" class="header-mark"></a>kNN-LM</h2><p>The following graph demonstrates the idea behind the $k\texttt{NN-LM}$ model.</p>
<figure><img src="/img/knn-lm.png" width="resize">
</figure>

<h3 id="data-preparation" class="headerLink">
    <a href="#data-preparation" class="header-mark"></a>Data Preparation</h3><p>To use the $k\texttt{NN-LM}$, we need to preprocess the documents in the corpus. The preprocessing procedure can be divided into some steps. Take the following sentence as an example.</p>]]></description>
</item><item>
    <title>Reading Notes: In-Context Retrieval-Augmented Language Models</title>
    <link>https://martinlwx.github.io/en/in-context-ralm-paper-reading/</link>
    <pubDate>Wed, 04 Dec 2024 00:53:25 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/in-context-ralm-paper-reading/</guid>
    <description><![CDATA[<h2 id="the-idea" class="headerLink">
    <a href="#the-idea" class="header-mark"></a>The idea</h2><p>In-Context RALM<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> is the RAG technology for Autoregressive LM. In summary, the RAG technology involves using a retriever during model inference to fetch relevant documents, which are then concatenated with the origin input.</p>
<p>In the In-Context Learning setting, some examples are placed <em>before</em> the user&rsquo;s input, and then they are fed to LLM. Similarly, the In-Context RALM works in a <em>similar</em> way: it directly concatenates <em>the most relevant retrieved document in front of</em> the model&rsquo;s input. The advantage is that there&rsquo;s <em>no need to retrain the LLM</em>. A diagram created with Mermaid is shown below.</p>]]></description>
</item><item>
    <title>Reading Notes: REALM: Retrieval-Augmented Language Model Pre-Training</title>
    <link>https://martinlwx.github.io/en/rag-realm-paper-reading/</link>
    <pubDate>Sat, 30 Nov 2024 00:42:53 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/rag-realm-paper-reading/</guid>
    <description><![CDATA[<h2 id="introduction" class="headerLink">
    <a href="#introduction" class="header-mark"></a>Introduction</h2><p>Recently I was planning to learn the RAG technology so I started to read some related papers. I found a good <a href="https://acl2023-retrieval-lm.github.io/slides/3-architecture.pdf" target="_blank" rel="noopener noreferrer">Roadmap</a> in the ACL 2023 Tutorial&rsquo;s slide. Today&rsquo;s topic is the most fundamental one: Retrieval-Augmented Language Model Pre-Training (REALM).</p>
<div class="details admonition info open">
    <div class="details-summary admonition-title">
        <span class="icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z"/></svg></span>Info<span class="details-icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></span>
    </div>
    <div class="details-content">
        <div class="admonition-content"><p>The REALM framework uses the Masked LM (BERT) rather than LLM. So I assume that you <em>have a basic understanding of BERT</em> such as how to do pre-train/fine-tuning.</p>]]></description>
</item></channel>
</rss>
