<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Internal - Tag - MartinLwx&#39;s blog</title>
        <link>https://martinlwx.github.io/en/tags/internal/</link>
        <description>Internal - Tag - MartinLwx&#39;s blog</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>martinlwx@163.com (MartinLwx)</managingEditor>
            <webMaster>martinlwx@163.com (MartinLwx)</webMaster><lastBuildDate>Thu, 14 Sep 2023 22:57:06 &#43;0800</lastBuildDate><atom:link href="https://martinlwx.github.io/en/tags/internal/" rel="self" type="application/rss+xml" /><item>
    <title>LoRA fine-tuning</title>
    <link>https://martinlwx.github.io/en/lora-finetuning/</link>
    <pubDate>Thu, 14 Sep 2023 22:57:06 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/en/lora-finetuning/</guid>
    <description><![CDATA[What&rsquo;s LoRA Since the era of LLM(large language model) arrived, fine-tuning LLM has become a challenge because the LLM models are extremely large, making it difficult to perform full fine-tuning. There are mainly two approaches: freeze the entire LLM and perform prompt tuning or In-context Learning; freeze the entire LLM but inserting trainable modules. Today, I will introduce the LoRA(Low-Rank Adaptation), which corresponds to the latter technical approach. This is a work proposed by the Microsoft team1]]></description>
</item>
<item>
    <title>Demystifying Pytorch&#39;s Strides Format</title>
    <link>https://martinlwx.github.io/en/how-to-reprensent-a-tensor-or-ndarray/</link>
    <pubDate>Fri, 14 Jul 2023 15:22:02 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/en/how-to-reprensent-a-tensor-or-ndarray/</guid>
    <description><![CDATA[Intro Even though I have been using Numpy and Pytorch for a long time, I never really knew how they implemented the underlying tensors and why they are so efficient. Recently, while studying the course Deep Learning Systems, I finally got the opportunity to try implementing tensors on my own. After going through the process, my understanding of tensors is much better ðŸ§
As a Pytorch user, is it necessary to understand the underlying tensor storage mechanism?]]></description>
</item>
</channel>
</rss>
