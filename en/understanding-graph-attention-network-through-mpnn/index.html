

<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="robots" content="noodp" />
    <title>Understanding GAT throught MPNN - MartinLwx&#39;s Blog</title><meta name="Description" content="Understanding the classic graph attention network(GAT) throught MPNN(Message passing neural network, MPNN)"><meta property="og:url" content="https://martinlwx.github.io/en/understanding-graph-attention-network-through-mpnn/">
  <meta property="og:site_name" content="MartinLwx&#39;s Blog">
  <meta property="og:title" content="Understanding GAT throught MPNN">
  <meta property="og:description" content="Understanding the classic graph attention network(GAT) throught MPNN(Message passing neural network, MPNN)">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
  <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-05-21T15:20:50+08:00">
    <meta property="article:modified_time" content="2023-05-21T15:20:50+08:00">
    <meta property="article:tag" content="GNN">
    <meta property="article:tag" content="Deep-Learning">
    <meta property="article:tag" content="Paper">
    <meta property="og:image" content="https://martinlwx.github.io/logo.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://martinlwx.github.io/logo.png"><meta name="twitter:title" content="Understanding GAT throught MPNN">
<meta name="twitter:description" content="Understanding the classic graph attention network(GAT) throught MPNN(Message passing neural network, MPNN)">
<meta name="application-name" content="MartinLwx&#39;s blog">
<meta name="apple-mobile-web-app-title" content="MartinLwx&#39;s blog">

<meta name="theme-color" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="canonical" href="https://martinlwx.github.io/en/understanding-graph-attention-network-through-mpnn/" /><link rel="prev" href="https://martinlwx.github.io/en/sicp-exercise-2-27/" /><link rel="next" href="https://martinlwx.github.io/en/git-bundle-tutorial/" />
<link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/normalize/normalize.min.css"><link rel="stylesheet" href="/css/color.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" as="style" onload="this.onload=null;this.rel='stylesheet'" href="/lib/fontawesome-free/all.min.css">
        <noscript><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"></noscript><link rel="preload" as="style" onload="this.onload=null;this.rel='stylesheet'" href="/lib/animate/animate.min.css">
        <noscript><link rel="stylesheet" href="/lib/animate/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Understanding GAT throught MPNN",
        "inLanguage": "en",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https://martinlwx.github.io/en/understanding-graph-attention-network-through-mpnn/"
        },"genre": "posts","keywords": "GNN, Deep-Learning, Paper","wordcount":  1351 ,
        "url": "https://martinlwx.github.io/en/understanding-graph-attention-network-through-mpnn/","datePublished": "2023-05-21T15:20:50+08:00","dateModified": "2023-05-21T15:20:50+08:00","license": "\u003ca rel=\"license noopener\" href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\" target=\"_blank\"\u003eCC BY-NC-ND 4.0\u003c/a\u003e","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "MartinLwx"
            },"description": "Understanding the classic graph attention network(GAT) throught MPNN(Message passing neural network, MPNN)"
    }
    </script></head>

<body header-desktop="fixed" header-mobile="auto"><script type="text/javascript">
        function setTheme(theme) {document.body.setAttribute('theme', theme); document.documentElement.style.setProperty('color-scheme', theme === 'light' ? 'light' : 'dark'); window.theme = theme;   window.isDark = window.theme !== 'light' }
        function saveTheme(theme) {window.localStorage && localStorage.setItem('theme', theme);}
        function getMeta(metaName) {const metas = document.getElementsByTagName('meta'); for (let i = 0; i < metas.length; i++) if (metas[i].getAttribute('name') === metaName) return metas[i]; return '';}
        if (window.localStorage && localStorage.getItem('theme')) {let theme = localStorage.getItem('theme');theme === 'light' || theme === 'dark' || theme === 'black' ? setTheme(theme) : (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches ? setTheme('dark') : setTheme('light')); } else { if ('auto' === 'light' || 'auto' === 'dark' || 'auto' === 'black') setTheme('auto'), saveTheme('auto'); else saveTheme('auto'), window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches ? setTheme('dark') : setTheme('light');}
        let metaColors = {'light': '#f8f8f8','dark': '#252627','black': '#000000'}
        getMeta('theme-color').content = metaColors[document.body.getAttribute('theme')];
        window.switchThemeEventSet = new Set()
    </script>
    <div id="back-to-top"></div>
    <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/en/" title="MartinLwx&#39;s Blog"><span id="desktop-header-typeit" class="typeit"></span></a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/en/posts/"> Posts </a><a class="menu-item" href="/en/tags/"> Tags </a><a class="menu-item" href="/en/categories/"> Categories </a><a class="menu-item" href="https://github.com/MartinLwx" title="GitHub" rel="noopener noreferrer" target="_blank"><i class='fab fa-github fa-fw'></i>  </a><span class="menu-item delimiter"></span><a href="javascript:void(0);" class="menu-item language" title="Select Language">English<i class="fas fa-chevron-right fa-fw"></i>
                        <select class="language-select" title="Select Language" id="language-select-desktop" onchange="location = this.value;"><option value="/en/understanding-graph-attention-network-through-mpnn/" selected>English</option><option value="/zh-cn/understanding-graph-attention-network-through-mpnn/">ÁÆÄ‰Ωì‰∏≠Êñá</option></select>
                    </a><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-select" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw"></i>
                    <select class="color-theme-select" id="theme-select-desktop" title="Switch Theme">
                        <option value="light">Light</option>
                        <option value="dark">Dark</option>
                        <option value="black">Black</option>
                        <option value="auto">Auto</option>
                    </select>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/en/" title="MartinLwx&#39;s Blog"><span id="mobile-header-typeit" class="typeit"></span></a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Cancel
                    </a>
                </div><a class="menu-item" href="/en/posts/" title="">Posts</a><a class="menu-item" href="/en/tags/" title="">Tags</a><a class="menu-item" href="/en/categories/" title="">Categories</a><a class="menu-item" href="https://github.com/MartinLwx" title="GitHub" rel="noopener noreferrer" target="_blank"><i class='fab fa-github fa-fw'></i></a><a href="javascript:void(0);" class="menu-item theme-select" title="Switch Theme">
                <i class="fas fa-adjust fa-fw"></i>
                <select class="color-theme-select" id="theme-select-mobile" title="Switch Theme">
                    <option value="light">Light</option>
                    <option value="dark">Dark</option>
                    <option value="black">Black</option>
                    <option value="auto">Auto</option>
                </select>
            </a><a href="javascript:void(0);" class="menu-item" title="Select Language">English<i class="fas fa-chevron-right fa-fw"></i>
                    <select class="language-select" title="Select Language" onchange="location = this.value;"><option value="/en/understanding-graph-attention-network-through-mpnn/" selected>English</option><option value="/zh-cn/understanding-graph-attention-network-through-mpnn/">ÁÆÄ‰Ωì‰∏≠Êñá</option></select>
                </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main">
            <div class="container"><div class="toc" id="toc-auto">
        <h2 class="toc-title">Contents</h2>
        <div class="toc-content" id="toc-content-auto"><nav id="TableOfContents">
  <ul>
    <li><a href="#whats-mpnn">What&rsquo;s MPNN</a></li>
    <li><a href="#whats-gat">What&rsquo;s GAT</a>
      <ul>
        <li>
          <ul>
            <li><a href="#step-1-apply-linear-transformation-to-all-nodes">Step 1. Apply linear transformation to all nodes</a></li>
            <li><a href="#step-2-compute-the-attention">Step 2. Compute the attention</a>
              <ul>
                <li><a href="#multi-head-attention">Multi-head attention</a></li>
              </ul>
            </li>
            <li><a href="#step-3-aggregation">Step 3. Aggregation</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#implementation-and-usage">Implementation and usage</a></li>
    <li><a href="#wrap-up">Wrap up</a></li>
    <li><a href="#refs">Refs</a></li>
  </ul>
</nav></div>
    </div><script>document.getElementsByTagName("main")[0].setAttribute("autoTOC", "true")</script><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Understanding GAT throught MPNN</h1><div class="post-meta">
            <div class="post-meta-line">
                <span class="post-author"><span class="author fas fa-user-circle fa-fw"></span><a href="https://github.com/MartinLwx" title="Author" target="_blank" rel="noopener noreferrer author" class="author">MartinLwx</a>
                </span>&nbsp;<span class="post-category">included in </span>&nbsp;<span class="post-category">category <a href="/en/categories/gnn/"><i class="far fa-folder fa-fw"></i>GNN</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2023-05-21">2023-05-21</time>&nbsp;<i class="far fa-edit fa-fw"></i>&nbsp;<time datetime="2023-05-21">2023-05-21</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;1351 words&nbsp;<i class="far fa-clock fa-fw"></i>&nbsp;7 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#whats-mpnn">What&rsquo;s MPNN</a></li>
    <li><a href="#whats-gat">What&rsquo;s GAT</a>
      <ul>
        <li>
          <ul>
            <li><a href="#step-1-apply-linear-transformation-to-all-nodes">Step 1. Apply linear transformation to all nodes</a></li>
            <li><a href="#step-2-compute-the-attention">Step 2. Compute the attention</a>
              <ul>
                <li><a href="#multi-head-attention">Multi-head attention</a></li>
              </ul>
            </li>
            <li><a href="#step-3-aggregation">Step 3. Aggregation</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#implementation-and-usage">Implementation and usage</a></li>
    <li><a href="#wrap-up">Wrap up</a></li>
    <li><a href="#refs">Refs</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><h2 id="whats-mpnn" class="headerLink">
    <a href="#whats-mpnn" class="header-mark"></a>What&rsquo;s MPNN</h2><p>Justin Gilmer proposed the MPNN (Message Passing Neural Network) framework <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> for describing graph neural network models used in supervised learning on graphs. I found this to be a useful framework that provides a clear understanding of how different GNN models work and facilitates a quick grasp of the differences between them. Considering a node $v$ on the graph $G$, the update procedure for its vector representation $h_v$ is as follows:</p>
<p>$$m_v^{t+1}=\sum_{u\in \mathcal{N}(v)}M_t(h_v^t,h_u^t,e_{vu})$$
$$h_v^{t+1}=U_t(h_v^t,m_v^{t+1})$$</p>
<p>where</p>
<ul>
<li>$u$ is the neighbor of $v$, and we use $\mathcal{N}(v)$ to represent all its neighbors</li>
<li>$e_{vu}$ is optional, which represents the edge feature</li>
<li>$M_t$ is the message function, $m_v^{t+1}$ is the aggregation result of all message from neighbors</li>
<li>$U_t$ is the vertex update function</li>
</ul>
<p>After updating the vector representations of all nodes on the graph, we may need to perform graph-level classification tasks, which correspond to the following formula in the MPNN framework:
$$\hat y=R({h_v^T|v\in G})$$</p>
<p>where</p>
<ul>
<li>$R$ is the readout function, which computes a feature vector for the whole graph (if you&rsquo;re doing a graph-level classification problem)</li>
</ul>
<h2 id="whats-gat" class="headerLink">
    <a href="#whats-gat" class="header-mark"></a>What&rsquo;s GAT</h2><blockquote>
<p>üßê I have found that linking the formulas with code can help with understanding. Therefore, I will provide relevant code(with <code>...</code> representing omitted parts). The code is sourced from the <a href="https://docs.dgl.ai/en/latest/_modules/dgl/nn/pytorch/conv/gatconv.html#GATConv" target="_blank" rel="noopener noreferrer">official <code>GATConv</code> module</a> in DGL.</p>
</blockquote>
<blockquote>
<p>üßê We can stack multiple GAT modules easily. The following discussion is from the perspective of a specific node $v$ in a particular layer $l$.</p>
</blockquote>
<h4 id="step-1-apply-linear-transformation-to-all-nodes" class="headerLink">
    <a href="#step-1-apply-linear-transformation-to-all-nodes" class="header-mark"></a>Step 1. Apply linear transformation to all nodes</h4><p>$$h_v^{l}=W^lh_v^{l}$$</p>
<p>Let&rsquo;s assume that the length of the vector representation for each node is denoted as $F$. In the first step, a linear transformation is applied to the vector of <strong>each node</strong> on the graph, where $W\in\mathcal{R}^{F&rsquo;\times F}$. Therefore, the length of each node is updated with a length of $F&rsquo;$. To distinguish vectors from different layers, superscript $l$ is used to indicate that it belongs to the $l$-th layer. Note that <strong>within the layer $l$, all nodes share the same weight matrix $W^l$</strong></p>
<blockquote>
<p>üìí <strong>Note that the $h_v^l$ or $h_u^l$ mentioned later have undergone linear transformations</strong>.</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">GATConv</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">...</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="o">...</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">_in_src_feats</span><span class="p">,</span> <span class="n">out_feats</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="o">...</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">graph</span><span class="p">,</span> <span class="n">feat</span><span class="p">,</span> <span class="o">...</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Args
</span></span></span><span class="line"><span class="cl"><span class="s2">        ----
</span></span></span><span class="line"><span class="cl"><span class="s2">            feat: (N, *, D_in) where D_in is the size of input feature
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Returns
</span></span></span><span class="line"><span class="cl"><span class="s2">        -------
</span></span></span><span class="line"><span class="cl"><span class="s2">            torch.Tensor
</span></span></span><span class="line"><span class="cl"><span class="s2">                (N, *, num_heads, D_out)
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="o">...</span>
</span></span><span class="line"><span class="cl">        <span class="n">src_prefix_shape</span> <span class="o">=</span> <span class="n">dst_prefix_shape</span> <span class="o">=</span> <span class="n">feat</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">h_src</span> <span class="o">=</span> <span class="n">h_dst</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feat_drop</span><span class="p">(</span><span class="n">feat</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># h_src: (N, *, D_in)</span>
</span></span><span class="line"><span class="cl">        <span class="n">feat_src</span> <span class="o">=</span> <span class="n">feat_dst</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">h_src</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="o">*</span><span class="n">src_prefix_shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_out_feats</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># feat_src/feat_dst: (N, *, num_heads, out_feat)</span>
</span></span><span class="line"><span class="cl">        <span class="o">...</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Note that in the above code, the presence of two identical <code>feat_src</code> and <code>feat_dst</code> variables in DGL is due to the adoption of a mathematically equivalent but computationally more efficient implementation. This will be explained later</p>
<h4 id="step-2-compute-the-attention" class="headerLink">
    <a href="#step-2-compute-the-attention" class="header-mark"></a>Step 2. Compute the attention</h4><p>$$e_{vu}^l=LeakyReLU\Big((a^l)^T[h_v^{l}||h_u^{l}]\Big)$$</p>
<p>$$\alpha_{vu}^l=Softmax_u(e_{vu}^l)$$</p>
<p>The second step is to compute the attention between the central node $v$ and all its neighboring nodes. In the above formula:</p>
<ul>
<li>$e_{vu}^l$ represents the attention coefficient. The paper mentions that different attention computation methods can be used. In the GAT paper, the authors chose to use a single-layer feedforward neural network(FNN) to compute the attention<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. <strong>Note that the $e_{vu}^l$ here is unrelated to $e_{vu}$ in MPNN; it just happens to have similar notation</strong></li>
<li>$||$ denotes the concatenation operation. It means that we concatenate the vector representations of the central node and its corresponding neighbor nodes, resulting in a vector of length $2F&rsquo;$, as indicated by $[h_v^{l}||h_u^{l}]$ in the formula. This concatenated vector is then fed into the aforementioned single-layer FNN, represented as $(a^l)^T[h_v^{l}||h_u^{l}]$, where $(a^l)^T$ refers to the learnable parameters of the single-layer FNN in the $l$-th layer</li>
<li>$LeakyReLU$ is the activation function</li>
<li>Finally, we apply Softmax on all neighbors of node $v$ to normalize the attention coefficients</li>
</ul>
<blockquote>
<p>ü§îÔ∏è Step 1 and 2 correspond to the computation of $m_v^{t+1}$ in the MPNN framework.</p>
</blockquote>
<h5 id="multi-head-attention" class="headerLink">
    <a href="#multi-head-attention" class="header-mark"></a>Multi-head attention</h5><p>Just as Transformers have multi-head attention, the authors of GAT also employ the mechanism of multi-head attention during node updates:
$$h_v^{l+1}= ||^{K^l} \sigma(\sum_{u\in\mathcal{N}(i)}\alpha_{vu}^{(k,l)}W^{(k,l)}h_u^{l})$$</p>
<p>The notations are getting more complex, but with careful consideration, they can still be understood. The superscript $(k,l)$ indicates the $k$-th head in the $l$-th layer. Here, $K^l$ represents the number of heads in the $l$-th layer. The meaning of the above formula is that each head will compute a vector representation, and these vectors from different heads will be concatenated together.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">GATConv</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">...</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="o">...</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">attn_l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">th</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">out_feats</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">attn_r</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">th</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">out_feats</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">leaky_relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">negative_slope</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="o">...</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">graph</span><span class="p">,</span> <span class="n">feat</span><span class="p">,</span> <span class="o">...</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Args
</span></span></span><span class="line"><span class="cl"><span class="s2">        ----
</span></span></span><span class="line"><span class="cl"><span class="s2">            feat: (N, *, D_in) where D_in is the size of input feature
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Returns
</span></span></span><span class="line"><span class="cl"><span class="s2">        -------
</span></span></span><span class="line"><span class="cl"><span class="s2">            torch.Tensor
</span></span></span><span class="line"><span class="cl"><span class="s2">                (N, *, num_heads, D_out)
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># feat_src/feat_dst: (N, *, num_heads, out_feat)</span>
</span></span><span class="line"><span class="cl">        <span class="n">el</span> <span class="o">=</span> <span class="p">(</span><span class="n">feat_src</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_l</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">er</span> <span class="o">=</span> <span class="p">(</span><span class="n">feat_dst</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_r</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># el/er: (N, *, num_heads, 1)</span>
</span></span><span class="line"><span class="cl">        <span class="n">graph</span><span class="o">.</span><span class="n">srcdata</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&#34;ft&#34;</span><span class="p">:</span> <span class="n">feat_src</span><span class="p">,</span> <span class="s2">&#34;el&#34;</span><span class="p">:</span> <span class="n">el</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">        <span class="n">graph</span><span class="o">.</span><span class="n">dstdata</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&#34;er&#34;</span><span class="p">:</span> <span class="n">er</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">        <span class="n">graph</span><span class="o">.</span><span class="n">apply_edges</span><span class="p">(</span><span class="n">fn</span><span class="o">.</span><span class="n">u_add_v</span><span class="p">(</span><span class="s2">&#34;el&#34;</span><span class="p">,</span> <span class="s2">&#34;er&#34;</span><span class="p">,</span> <span class="s2">&#34;e&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">e</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">leaky_relu</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">edata</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&#34;e&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># e: (N, *, num_heads, 1)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># normalization</span>
</span></span><span class="line"><span class="cl">        <span class="n">graph</span><span class="o">.</span><span class="n">edata</span><span class="p">[</span><span class="s2">&#34;a&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_drop</span><span class="p">(</span><span class="n">edge_softmax</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">e</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># a: (N, *, num_heads, 1)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># weighted sum</span>
</span></span><span class="line"><span class="cl">        <span class="n">graph</span><span class="o">.</span><span class="n">update_all</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># ft: (N, *, num_heads, out_feat)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># a: (N, *, num_heads, 1)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># m: (N, *, num_heads, out_feat)</span>
</span></span><span class="line"><span class="cl">            <span class="n">fn</span><span class="o">.</span><span class="n">u_mul_e</span><span class="p">(</span><span class="s2">&#34;ft&#34;</span><span class="p">,</span> <span class="s2">&#34;a&#34;</span><span class="p">,</span> <span class="s2">&#34;m&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">            <span class="n">fn</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s2">&#34;m&#34;</span><span class="p">,</span> <span class="s2">&#34;ft&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">rst</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">dstdata</span><span class="p">[</span><span class="s2">&#34;ft&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># rst: (N, *, num_heads, out_feat)</span>
</span></span><span class="line"><span class="cl">        <span class="o">...</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>The implementation of <code>GATConv</code> in DGL is based on this equation:
$$a^T[h_v||h_u]=a_l^Th_v+a_r^Th_u$$</p>
<p>Why it is more efficient?</p>
<ol>
<li>We don&rsquo;t need to store $[h_v||h_u]$ on edges (DGL will store messages in edge)</li>
<li>The addition could be optimized with DGL&rsquo;s built-in function <code>u_add_v</code></li>
</ol>
<h4 id="step-3-aggregation" class="headerLink">
    <a href="#step-3-aggregation" class="header-mark"></a>Step 3. Aggregation</h4><p>The final way of updating the nodes is by taking the weighted sum of the neighbor&rsquo;s vector representations using the corresponding attention scores. It can be expressed as follows:
$$h_v^{l+1}=\sigma(\sum_{u\in \mathcal{N}(i)}\alpha_{vu}W^{l}h_u^{l})$$</p>
<blockquote>
<p>ü§îÔ∏è The above corresponds to the computation of $h_v^{t+1}$ in the MPNN framework. It is worth noting that when calculating $h_t^{l+1}$ in GAT, the previous layer representation $h_t^l$ is not used. Additionally, GAT was proposed to address node classification problems on graphs and does not involve any graph readout operations.</p>
</blockquote>
<p>In the GAT paper, the authors intended to use GAT for node-level classification tasks<sup id="fnref1:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Suppose we stack $L$ layers of GAT. It would be unreasonable to use concatenation in the final(prediction) layer. Therefore, in the last GAT layer, the authors take the average of multiple heads before applying the activation function. If the activation function used here is Softmax, it can be directly used for node classification<sup id="fnref2:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. The formula is as follows:
$$final\ embedding\ of\ h_v= \sigma\Big(\frac{1}{K^L}\sum_{k=1}^{K^L}\sum_{u\in\mathcal{N}(i)}\alpha_{vu}^{(k,L)}W^{(k,L)}h_u^{L}\Big)$$</p>
<h2 id="implementation-and-usage" class="headerLink">
    <a href="#implementation-and-usage" class="header-mark"></a>Implementation and usage</h2><blockquote>
<p>ü§îÔ∏è <a href="https://www.dgl.ai/" target="_blank" rel="noopener noreferrer">DGL</a>&rsquo;s design is based on the MPNN framework, but their formulas are slightly different. They also introduce an aggregation function, denoted as $\rho$, which determines how a node aggregates all the information received from its neighbors. <strong>I thought their formulas are more generalized</strong>. They have thoughtfully provided a tutorial on how to use DGL&rsquo;s MPNN-related functions, which can be found <a href="https://docs.dgl.ai/en/latest/guide/message.html" target="_blank" rel="noopener noreferrer">here</a> üëçüëçüëç</p>
</blockquote>
<p>As for the implementation of GAT, the DGL offers <a href="https://docs.dgl.ai/en/latest/generated/dgl.nn.pytorch.conv.GATConv.html#dgl.nn.pytorch.conv.GATConv" target="_blank" rel="noopener noreferrer">GATConv</a>. The DGL team also write a good tutorial about using the built-in <code>message_func</code> and <code>reduce_func</code> to <a href="https://docs.dgl.ai/en/latest/tutorials/models/1_gnn/9_gat.html" target="_blank" rel="noopener noreferrer">implemente GAT manually</a></p>
<p>Please refers to <a href="https://github.com/dmlc/dgl/blob/master/examples/core/gat/train.py" target="_blank" rel="noopener noreferrer">here</a> to see a full training example</p>
<h2 id="wrap-up" class="headerLink">
    <a href="#wrap-up" class="header-mark"></a>Wrap up</h2><p>Above is how the GAT can be explained using the MPNN framework, with the inclusion of DGL source code. Using attention to compute importances between nodes appears to be a natural approach and can be seen as a generalization of GCN. GAT is capable of learning local structural representations of graphs effectively, and the attention computation can be parallelized, making it highly efficient. Cheers! üçªüçªüçª</p>
<h2 id="refs" class="headerLink">
    <a href="#refs" class="header-mark"></a>Refs</h2><div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Gilmer J, Schoenholz S S, Riley P F, et al. Neural message passing for quantum chemistry[C]//International conference on machine learning. PMLR, 2017: 1263-1272. <a href="https://arxiv.org/abs/1704.01212" target="_blank" rel="noopener noreferrer">arXiv</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Veliƒçkoviƒá P, Cucurull G, Casanova A, et al. Graph attention networks[J]. arXiv preprint arXiv:1710.10903, 2017. <a href="https://arxiv.org/abs/1710.10903" target="_blank" rel="noopener noreferrer">arXiv</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
</div>

        <div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2023-05-21</span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share"><button title="Share on Twitter" data-sharer="twitter" data-url="https://martinlwx.github.io/en/understanding-graph-attention-network-through-mpnn/" data-title="Understanding GAT throught MPNN" data-hashtags="GNN,Deep-Learning,Paper"><span class="fab fa-twitter fa-fw"></span></button><button title="Share on Facebook" data-sharer="facebook" data-url="https://martinlwx.github.io/en/understanding-graph-attention-network-through-mpnn/" data-hashtag="GNN"><span class="fab fa-facebook-square fa-fw"></span></button><button title="Share on Hacker News" data-sharer="hackernews" data-url="https://martinlwx.github.io/en/understanding-graph-attention-network-through-mpnn/" data-title="Understanding GAT throught MPNN"><span class="fab fa-hacker-news fa-fw"></span></button><button title="Share on Line" data-sharer="line" data-url="https://martinlwx.github.io/en/understanding-graph-attention-network-through-mpnn/" data-title="Understanding GAT throught MPNN"><span data-svg-src="/lib/simple-icons/icons/line.min.svg"></span></button><button title="Share on ÂæÆÂçö" data-sharer="weibo" data-url="https://martinlwx.github.io/en/understanding-graph-attention-network-through-mpnn/" data-title="Understanding GAT throught MPNN"><span class="fab fa-weibo fa-fw"></span></button><button title="Share on Telegram" data-sharer="telegram" data-url="https://martinlwx.github.io/en/understanding-graph-attention-network-through-mpnn/" data-title="Understanding GAT throught MPNN" data-web><span class="fab fa-telegram-plane fa-fw"></span></button><script>
        function shareOnMastodon(title, link) {
            const SHARE_MASTODON_DOMAIN = "share_mastodon_domain"
            const savedDomain = localStorage.getItem(SHARE_MASTODON_DOMAIN) ?? "mastodon.social";
            const domain = prompt("Enter your Mastodon domain", savedDomain);
            if (domain === null) {
                return;
            }
            localStorage.setItem(SHARE_MASTODON_DOMAIN, domain)
            const text = title + "\n\n" + link;
            const url = new URL("https://" + domain)
            url.pathname = "share"
            url.searchParams.append('text', text)
            window.open(url, '_blank', "width=500,height=500,left=500,toolbar=0,status=0");
        }
    </script>
    <button title="Share on Mastodon"onclick="javascript:shareOnMastodon(&#34;Understanding GAT throught MPNN&#34;, &#34;https://martinlwx.github.io/en/understanding-graph-attention-network-through-mpnn/&#34;)"><span class="fab fa-mastodon fa-fw"></span></button></div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw"></i>&nbsp;<a href="/en/tags/gnn/">GNN</a>,&nbsp;<a href="/en/tags/deep-learning/">Deep-Learning</a>,&nbsp;<a href="/en/tags/paper/">Paper</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/en/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/en/sicp-exercise-2-27/" class="prev" rel="prev" title="SICP Exercise 2.27"><i class="fas fa-angle-left fa-fw"></i>SICP Exercise 2.27</a>
            <a href="/en/git-bundle-tutorial/" class="next" rel="next" title="Git bundle guide">Git bundle guide<i class="fas fa-angle-right fa-fw"></i></a></div>
</div>
<div id="comments"><div id="giscus"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://giscus.app/">giscus</a>.
            </noscript></div></article></div>
        </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">
                    Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener noreferrer" title="Hugo 0.125.1">Hugo</a>&nbsp;|&nbsp;Theme - <a href="https://github.com/HEIGE-PCloud/DoIt" target="_blank" rel="noopener noreferrer" title="DoIt 0.4.0"><i class="far fa-edit fa-fw"></i> DoIt</a>
                </div><div class="footer-line"><i class="far fa-copyright fa-fw"></i><span itemprop="copyrightYear">2019 - 2024</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="https://github.com/MartinLwx" target="_blank" rel="noopener noreferrer">MartinLwx</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
            <div class="footer-line"></div>
            <div class="footer-line">
            </div>
        </div></footer></div>

    <div id="fixed-buttons"><a href="#back-to-top" id="back-to-top-button" class="fixed-button" title="Back to Top">
            <i class="fas fa-arrow-up fa-fw"></i>
        </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
            <i class="fas fa-comment fa-fw"></i>
        </a>
    </div><div class="assets"><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="preload" as="style" onload="this.onload=null;this.rel='stylesheet'" href="/lib/katex/copy-tex.min.css">
        <noscript><link rel="stylesheet" href="/lib/katex/copy-tex.min.css"></noscript><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":25},"comment":{"giscus":{"darkTheme":"dark","dataCategory":"Announcements","dataCategoryId":"DIC_kwDOGfB5nc4Ccjd0","dataEmitMetadata":"0","dataInputPosition":"bottom","dataLang":"en","dataLoading":"lazy","dataMapping":"pathname","dataReactionsEnabled":"1","dataRepo":"MartinLwx/martinlwx.github.io","dataRepoId":"R_kgDOGfB5nQ","dataStrict":"0","lightTheme":"preferred_color_scheme"}},"data":{"desktop-header-typeit":"MartinLwx's blog","mobile-header-typeit":"MartinLwx's blog"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"O4EMEES3VS","algoliaIndex":"index.en","algoliaSearchKey":"36fdaa364fae083332d0c8709026bd62","highlightTag":"em","maxResultLength":10,"noResultsFound":"No results found","snippetLength":30,"type":"algolia"},"sharerjs":true,"table":{"sort":true},"typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"desktop-header-typeit":["desktop-header-typeit"],"mobile-header-typeit":["mobile-header-typeit"]},"duration":-1,"speed":100}};</script><script type="text/javascript" src="/lib/tablesort/tablesort.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="/lib/sharer/sharer.min.js"></script><script type="text/javascript" src="/lib/typeit/typeit.min.js"></script><script type="text/javascript" src="/lib/katex/katex.min.js" defer></script><script type="text/javascript" src="/lib/katex/auto-render.min.js" defer></script><script type="text/javascript" src="/lib/katex/copy-tex.min.js" defer></script><script type="text/javascript" src="/lib/katex/mhchem.min.js" defer></script><script type="text/javascript" src="/js/katex.min.js" defer></script><script type="text/javascript" src="/js/theme.min.js" defer></script><script type="text/javascript" src="/js/giscus.min.js" defer></script><script type="text/javascript">
            window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());
            gtag('config', 'G-7RY6742J2F', { 'anonymize_ip': true });
        </script><script type="text/javascript" src="https://www.googletagmanager.com/gtag/js?id=G-7RY6742J2F" async></script></div>
</body>

</html>
