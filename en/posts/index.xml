<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>All Posts - MartinLwx&#39;s blog</title>
        <link>https://martinlwx.github.io/en/posts/</link>
        <description>All Posts | MartinLwx&#39;s blog</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>martinlwx@163.com (MartinLwx)</managingEditor>
            <webMaster>martinlwx@163.com (MartinLwx)</webMaster><lastBuildDate>Wed, 16 Aug 2023 22:23:26 &#43;0800</lastBuildDate><atom:link href="https://martinlwx.github.io/en/posts/" rel="self" type="application/rss+xml" /><item>
    <title>TF-IDF model</title>
    <link>https://martinlwx.github.io/en/an-introduction-of-tf-idf-model/</link>
    <pubDate>Wed, 16 Aug 2023 22:23:26 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/en/an-introduction-of-tf-idf-model/</guid>
    <description><![CDATA[What is the TF-IDF model In previous post, we talked about the bag-of-word model, which has many limitations. Today we take a step further to see if we can try to fix one of the limitations - Each word has the same importance.
ðŸ’¡ The crux of the problem - How to define the word importance?
One idea is: The more frequently a word appears within a single document, the more important it is for that document.]]></description>
</item>
<item>
    <title>An Introduction of Bag of Word Model</title>
    <link>https://martinlwx.github.io/en/an-introduction-of-bag-of-word-model/</link>
    <pubDate>Fri, 11 Aug 2023 18:55:09 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/en/an-introduction-of-bag-of-word-model/</guid>
    <description><![CDATA[What is the bag-of-word model? In NLP, we need to represent each document as a vector because machine learning can only accept input as numbers. That is, we want to find a magic function that: $$ f(\text{document}) = vector $$
Today&rsquo;s topic is bag-of-word(BoW) model, which can transform a document into a vector representation.
ðŸ’¡ Although the BoW model is outdated in 2023, I still encourage you to learn from the history and think about some essential problems:]]></description>
</item>
<item>
    <title>A trick to calculating partial derivatives in machine learning</title>
    <link>https://martinlwx.github.io/en/a-trick-to-calculating-partial-derivatives-in-ml/</link>
    <pubDate>Wed, 26 Jul 2023 00:31:50 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/en/a-trick-to-calculating-partial-derivatives-in-ml/</guid>
    <description><![CDATA[Intro You may have difficulties when trying to calculate the partial derivatives in machine learning like me. Even though I found a good reference cookbook that could be used to derive the gradients, I still got confused. Today, I want to share a practical technique I recently learned from this video: when calculating partial derivatives in machine learning, you can treat everything as if it were a scalar and then make the shapes match]]></description>
</item>
<item>
    <title>Demystifying Pytorch&#39;s Strides Format</title>
    <link>https://martinlwx.github.io/en/how-to-reprensent-a-tensor-or-ndarray/</link>
    <pubDate>Fri, 14 Jul 2023 15:22:02 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/en/how-to-reprensent-a-tensor-or-ndarray/</guid>
    <description><![CDATA[Intro Even though I have been using Numpy and Pytorch for a long time, I never really knew how they implemented the underlying tensors and why they are so efficient. Recently, while studying the course Deep Learning Systems, I finally got the opportunity to try implementing tensors on my own. After going through the process, my understanding of tensors is much better ðŸ§
As a Pytorch user, is it necessary to understand the underlying tensor storage mechanism?]]></description>
</item>
<item>
    <title>How to memorize the Red-black tree</title>
    <link>https://martinlwx.github.io/en/how-to-memorize-insertion-and-deletion-in-rb-tree/</link>
    <pubDate>Sat, 01 Jul 2023 17:12:40 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/en/how-to-memorize-insertion-and-deletion-in-rb-tree/</guid>
    <description><![CDATA[Intro If you are attracted by the title of this blog, I believe you may agree with me: The process of memorizing the insertion and deletion operations of the Red-black tree can be incredibly arduous. It entails keeping track of complex tree rotations and the necessity to recolor nodes as required. I once read the renowned Introducing to Algorithms written by the CLRS. However, there are so many cases to remember and I quickly get overwhelmed.]]></description>
</item>
<item>
    <title>Git bundle guide</title>
    <link>https://martinlwx.github.io/en/git-bundle-tutorial/</link>
    <pubDate>Fri, 16 Jun 2023 23:48:28 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/en/git-bundle-tutorial/</guid>
    <description><![CDATA[What is the git bundle command git bundle is a relatively less commonly used git command. Its purpose is to package a git repo into a single file, which can then be used by others to recreate the original git repo. Additionally, git bundle supports incremental update. Before I learned about the git bundle command, I would usually directly use tar czf some_git_repo to create a package for a git repo.]]></description>
</item>
<item>
    <title>Understanding GAT throught MPNN</title>
    <link>https://martinlwx.github.io/en/understanding-graph-attention-network-through-mpnn/</link>
    <pubDate>Sun, 21 May 2023 15:20:50 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/en/understanding-graph-attention-network-through-mpnn/</guid>
    <description><![CDATA[What&rsquo;s MPNN Justin Gilmer proposed the MPNN (Message Passing Neural Network) framework 1 for describing graph neural network models used in supervised learning on graphs. I found this to be a useful framework that provides a clear understanding of how different GNN models work and facilitates a quick grasp of the differences between them. Considering a node $v$ on the graph $G$, the update procedure for its vector representation $h_v$ is as follows:]]></description>
</item>
<item>
    <title>SICP Exercise 2.27</title>
    <link>https://martinlwx.github.io/en/sicp-exercise-2-27/</link>
    <pubDate>Tue, 16 May 2023 12:41:20 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/en/sicp-exercise-2-27/</guid>
    <description><![CDATA[Question Modify your reverse procedure of exercise 2.18 to produce a deep-reverse procedure that takes a list as an argument and returns as its value the list with its elements reversed and with all sublists deep-reversed as well.
1 2 3 4 (define x (list (list 1 2) (list 3 4))) ;; x - ((1 2) (3 4)) (deep-reverse x) ;; the output should be ((4 3) (2 1)) Answer In the previous Exercise 2.]]></description>
</item>
<item>
    <title>SICP Exercise 1.46</title>
    <link>https://martinlwx.github.io/en/sicp-exercise-1-46/</link>
    <pubDate>Wed, 10 May 2023 13:40:48 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/en/sicp-exercise-1-46/</guid>
    <description><![CDATA[Question Several of the numerical methods described in this chapter are instances of an extremely general computational strategy known as iterative improvement. Iterative improvement says that, to compute something, we start with an initial guess for the answer, test if the guess is good enough, and otherwise improve the guess and continue the process using the improved guess as the new guess. Write a procedure iterative-improve that takes two procedures as arguments: a method for telling whether a guess is good enough and a method for improving a guess.]]></description>
</item>
<item>
    <title>SICP Exercise 1.34</title>
    <link>https://martinlwx.github.io/en/sicp-exercise-1-34/</link>
    <pubDate>Tue, 09 May 2023 14:06:18 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/en/sicp-exercise-1-34/</guid>
    <description><![CDATA[Question Suppose we define the procedure f. What happens if we (perversely) ask the interpreter to evaluate the combination (f f)?
1 2 3 4 5 (define (square x) (* x x)) (define (f g) (g 2)) Then we have
1 2 3 (f square) ;; 4 (f (lambda (z) (* z (+ z 1)))) ;; 6 = 2 * 3 Answer Recall what the applicative-order evaluation says: We need to evaluate all arguments and then we apply procedure on these arguments.]]></description>
</item>
</channel>
</rss>
