<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>MartinLwx&#39;s Blog</title>
        <link>https://martinlwx.github.io/en/</link>
        <description>Welcome to my blog :)</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>martinlwx@163.com (MartinLwx)</managingEditor>
            <webMaster>martinlwx@163.com (MartinLwx)</webMaster><copyright>&lt;a rel=&#34;license noopener&#34; href=&#34;https://creativecommons.org/licenses/by-nc-nd/4.0/&#34; target=&#34;_blank&#34;&gt;CC BY-NC-ND 4.0&lt;/a&gt;</copyright><lastBuildDate>Sun, 11 Jan 2026 22:23:22 &#43;0800</lastBuildDate>
            <atom:link href="https://martinlwx.github.io/en/index.xml" rel="self" type="application/rss+xml" />
        <item>
    <title>Suffix array: find the needle in the hay</title>
    <link>https://martinlwx.github.io/en/suffix-array-tutorial/</link>
    <pubDate>Sun, 11 Jan 2026 22:23:22 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/suffix-array-tutorial/</guid>
    <description><![CDATA[<h2 id="suffix-array" class="headerLink">
    <a href="#suffix-array" class="header-mark"></a>Suffix array</h2><p>By definition, a suffix array (denoted as <code>sa</code>) contains the starting indices of all suffixes. It&rsquo;s simply an <code>int</code> array where <code>sa[i]</code> represents the starting index of the corresponding suffix.</p>
<p>Taking <code>fizzbuzz</code> as an example, its suffix array is <code>4 0 1 5 7 3 6 2</code>. The details are shown in the following table.</p>
<table>
  <thead>
      <tr>
          <th>Suffix array <code>sa</code></th>
          <th>Corresponding suffix</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>4</td>
          <td><code>buzz</code></td>
      </tr>
      <tr>
          <td>0</td>
          <td><code>fizzbuzz</code></td>
      </tr>
      <tr>
          <td>1</td>
          <td><code>izzbuzz</code></td>
      </tr>
      <tr>
          <td>5</td>
          <td><code>uzz</code></td>
      </tr>
      <tr>
          <td>7</td>
          <td><code>z</code></td>
      </tr>
      <tr>
          <td>3</td>
          <td><code>zbuzz</code></td>
      </tr>
      <tr>
          <td>6</td>
          <td><code>zz</code></td>
      </tr>
      <tr>
          <td>2</td>
          <td><code>zzbuzz</code></td>
      </tr>
  </tbody>
</table>
<h2 id="how-to-generate-a-suffix-array" class="headerLink">
    <a href="#how-to-generate-a-suffix-array" class="header-mark"></a>How to generate a suffix array</h2><h3 id="naive-way" class="headerLink">
    <a href="#naive-way" class="header-mark"></a>Naive way</h3><p>Let $N$ represent the length of the string. The naive algorithm is straightforward: generate all possible suffixes and then sort them using an efficient sorting algorithm. This requires one sort iteration, which may involve $O(N log\ N)$ comparisons. In the worst case, each string comparison takes $O(N)$ time. Therefore, the overall time complexity would be</p>]]></description>
</item><item>
    <title>Better TF-IDF: BM25</title>
    <link>https://martinlwx.github.io/en/better-tf-idf-bm25/</link>
    <pubDate>Tue, 23 Sep 2025 22:00:13 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/better-tf-idf-bm25/</guid>
    <description><![CDATA[<h2 id="intro" class="headerLink">
    <a href="#intro" class="header-mark"></a>Intro</h2><p>You probably encountered BM25 numerous times when reading papers from the LLM (RAG) or information retrieval domain. This algorithm is a ranking method that computes relevance scores for documents given a user query.</p>
<p>If you examine the BM25 formula carefully, you will notice its similarity to the classic TF-IDF. In fact, BM25 is <em>an enhanced version</em> of TF-IDF, as we&rsquo;ll explore shortly.</p>
<h2 id="the-bm25" class="headerLink">
    <a href="#the-bm25" class="header-mark"></a>The BM25</h2><p>Before we dive into the mechanism of BM25, let&rsquo;s establish some notations:</p>]]></description>
</item><item>
    <title>t-SNE &#43; K-Means: Data visualization and Clustering</title>
    <link>https://martinlwx.github.io/en/k-means-tutorial/</link>
    <pubDate>Fri, 12 Sep 2025 23:53:24 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/k-means-tutorial/</guid>
    <description><![CDATA[<h2 id="k-means-algorithm" class="headerLink">
    <a href="#k-means-algorithm" class="header-mark"></a>K-Means Algorithm</h2><p>I recently used the K-Means algorithm for clustering in my work. While reviewing my notes on the topic, I decided to publish them online :)</p>
<p>K-Means is a clustering algorithm that assigns each sample to one of the $K$ clusters.</p>
<h2 id="how-does-ak-means-algorithm-work" class="headerLink">
    <a href="#how-does-ak-means-algorithm-work" class="header-mark"></a>How does AK-Means algorithm work?</h2><h3 id="data-preparation" class="headerLink">
    <a href="#data-preparation" class="header-mark"></a>Data Preparation</h3><p>K-Means algorithm relies on distance calculations, so data should be normalized to prevent features with larger scales from dominating the results. The normalization can be achieved by the <code>Scikit-Learn</code> library as follows.</p>]]></description>
</item><item>
    <title>Association Rule Mining: the Apriori Algorithm</title>
    <link>https://martinlwx.github.io/en/use-apriori-algorithm-for-association-rule-mining/</link>
    <pubDate>Wed, 30 Jul 2025 23:03:58 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/use-apriori-algorithm-for-association-rule-mining/</guid>
    <description><![CDATA[<h2 id="introduction" class="headerLink">
    <a href="#introduction" class="header-mark"></a>Introduction</h2><p>In my recent work, I&rsquo;ve been analyzing the correlation between various features in Android APKs. These features include IP characteristics, URL attributes, permission settings, and more. Feature correlation refers to identifying relationships between different features from the data, such as certain feature combinations that frequently appear together.</p>
<p>Relying solely on manual analysis would be impractical given the massive datasetâ€”that&rsquo;s when I remembered an algorithm from my data mining course: the Apriori algorithm! :)</p>]]></description>
</item><item>
    <title>Async &#43; Leaky Bucket: How to Batch LLM API Calls Efficiently</title>
    <link>https://martinlwx.github.io/en/async-and-leaky-bucket-algorithm-batch-llm-api-call/</link>
    <pubDate>Wed, 18 Jun 2025 23:07:55 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/async-and-leaky-bucket-algorithm-batch-llm-api-call/</guid>
    <description><![CDATA[<h2 id="background" class="headerLink">
    <a href="#background" class="header-mark"></a>Background</h2><p>Recently, at work, I&rsquo;ve been working on setting up a LLM evaluation platform. There&rsquo;s one particular scenario: we need to call an LLM API provided by another department to run model evaluations on a test dataset, but this LLM API has a rate limit of a maximum of 2 calls per second (2 RPS). Thus, my task essentially boils down to: <em>How to maximize concurrency to speed up model evaluation while strictly adhering to the API rate limits</em>. In this brief post, I will share my thoughts about approaching this task.</p>]]></description>
</item><item>
    <title>Programming with Categories: Functor</title>
    <link>https://martinlwx.github.io/en/why-functor/</link>
    <pubDate>Sun, 01 Jun 2025 22:50:47 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/why-functor/</guid>
    <description><![CDATA[<h2 id="intro" class="headerLink">
    <a href="#intro" class="header-mark"></a>Intro</h2><p>What&rsquo;s a functor? You might use it daily without realizing it. For example, calling <code>map</code> on a collection means you&rsquo;re using a functor.</p>
<p>This post explains functors from two perspectives: category theory and programming. By this end, you will have a deeper understanding of the concept.</p>
<h2 id="functor-in-category-theory" class="headerLink">
    <a href="#functor-in-category-theory" class="header-mark"></a>Functor in category theory</h2><div class="details admonition warning open">
    <div class="details-summary admonition-title">
        <span class="icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"/></svg></span>Warning<span class="details-icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></span>
    </div>
    <div class="details-content">
        <div class="admonition-content"><p>This section assumes you know what a category is. If you haven&rsquo;t heard of it before, think of a category as a collection of objects and the relationships between them.</p>]]></description>
</item><item>
    <title>Transformer architecture variation: Rotary Position Embedding (RoPE)</title>
    <link>https://martinlwx.github.io/en/rotary-position-embedding/</link>
    <pubDate>Sat, 24 May 2025 16:28:13 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/rotary-position-embedding/</guid>
    <description><![CDATA[<h2 id="a-recap-of-self-attention-mechanism" class="headerLink">
    <a href="#a-recap-of-self-attention-mechanism" class="header-mark"></a>A Recap of Self-attention Mechanism</h2><p>In self-attention, the query ($\mathbf q_m$), key ($\mathbf k_n$), and value ($\mathbf v_n$) vectors are computed as follows:</p>
<p>$$
\begin{aligned}
\mathbf q_m&amp;=f_q(\mathbf x_m,m)\\
\mathbf k_n&amp;=f_k(\mathbf x_n,n)\\
\mathbf v_n&amp;=f_v(\mathbf x_n,n)
\end{aligned}
$$</p>
<p>Here, the $\mathbf x_i$ is the $i$-th token embedding, while $n$ and $m$ denote different positions.</p>
<p>The attention score between position $m$ and $n$ is computed as:</p>
<p>$$
\alpha_{m,n}=\frac{exp(\frac{\mathbf q_m^T\mathbf k_n}{\sqrt d})}{\sum_{j=1}^Nexp(\frac{\mathbf q_m^T\mathbf k_j}{\sqrt d})}
$$</p>]]></description>
</item><item>
    <title>Transformer architecture variation: RMSNorm</title>
    <link>https://martinlwx.github.io/en/rmsnorm-in-a-nutshell/</link>
    <pubDate>Sun, 11 May 2025 14:01:26 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/rmsnorm-in-a-nutshell/</guid>
    <description><![CDATA[<h2 id="intro" class="headerLink">
    <a href="#intro" class="header-mark"></a>Intro</h2><p>It&rsquo;s been 8 years since the famous transformer architecture was first proposed. You might have noticed that some modifications to the original design - for instance, most large language models (LLMs) now use RMSNorm<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> instead of LayerNorm. Today I will briefly introduce RMSNorm, but first, let&rsquo;s recap LayerNorm.</p>
<h2 id="layernorm-recap" class="headerLink">
    <a href="#layernorm-recap" class="header-mark"></a>LayerNorm Recap</h2><p>$$
\mathbf y=\frac{\mathbf x-E[\mathbf x]}{\sqrt{Var(\mathbf x)+\epsilon}}*\gamma+\beta
$$</p>
<p>The equation above shows how LayerNorm works. If we ignore the scaling factors ($\gamma, \beta$), LayerNorm&rsquo;s behavior becomes intuitive: <em>it transforms each input $\mathbf x$ into a feature vector with zero mean and unit standard deviation .</em></p>]]></description>
</item><item>
    <title>Kosaraju&#39;s Algorithm Explained</title>
    <link>https://martinlwx.github.io/en/kosaraju-algorithm-explained/</link>
    <pubDate>Sat, 26 Apr 2025 17:23:49 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/kosaraju-algorithm-explained/</guid>
    <description><![CDATA[<h2 id="intro" class="headerLink">
    <a href="#intro" class="header-mark"></a>Intro</h2><p>During my daily coding practice, I encountered an interesting problem - <a href="https://cses.fi/problemset/task/1682/" target="_blank" rel="noopener noreferrer">1682. Flight Routes Check</a>. Solving this problem requires finding all strongly connected components (SCCs) in a directed graph. After some research, I discovered Kosaraju&rsquo;s algorithm, which solves this problem <em>in linear time</em>. That is, the time complexity is</p>
<p>$$
O(V+E)
$$</p>
<p>Where $V$ refers to the nodes and $E$ refers to the edges in the graph.</p>
<p>By interesting, I mean that Kosaraju&rsquo;s algorithm is <em>easy to implement</em> yet <em>a bit tricky to understand fully</em>. In my opinion, knowing why it works matters more than just memorizing how to code it. That&rsquo;s why I&rsquo;m sharing this short post - to break down the key insights.</p>]]></description>
</item><item>
    <title>One for all: the torch.einsum API</title>
    <link>https://martinlwx.github.io/en/the-magic-torch-einsum-api/</link>
    <pubDate>Mon, 14 Apr 2025 20:44:01 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/the-magic-torch-einsum-api/</guid>
    <description><![CDATA[<h2 id="motivations" class="headerLink">
    <a href="#motivations" class="header-mark"></a>Motivations</h2><p>In PyTorch, <em>multiple APIs</em> exist for matrix multiplication operations. However, these functions often lead to memorization challenges. Additionally, many of these APIs <em>require explicit dimension manipulation</em> (e.g., permuting, reshaping)</p>
<p>Does there exist a <em>magic</em> API that can cover all the use cases? A potential unified solution is the <code>torch.einsum</code> API.</p>
<h2 id="what-is-torcheinsum-" class="headerLink">
    <a href="#what-is-torcheinsum-" class="header-mark"></a>What is torch.einsum ?</h2><p>The syntax of <code>torch.einsum</code> is</p>
<div class="code-block highlight is-open show-line-numbers  tw-group tw-my-2">
  <div class="
    
    tw-flex 
    tw-flex-row
    tw-flex-1 
    tw-justify-between 
    tw-w-full tw-bg-bgColor-secondary
    ">      
    <button 
      class="
        code-block-button
        tw-mx-2 
        tw-flex
        tw-flex-row
        tw-flex-1"
      aria-hidden="true">
          <div class="group-[.is-open]:tw-rotate-90 tw-transition-[transform] tw-duration-500 tw-ease-in-out print:!tw-hidden tw-w-min tw-h-min tw-my-1 tw-mx-1"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M285.476 272.971L91.132 467.314c-9.373 9.373-24.569 9.373-33.941 0l-22.667-22.667c-9.357-9.357-9.375-24.522-.04-33.901L188.505 256 34.484 101.255c-9.335-9.379-9.317-24.544.04-33.901l22.667-22.667c9.373-9.373 24.569-9.373 33.941 0L285.475 239.03c9.373 9.372 9.373 24.568.001 33.941z"/></svg></div>
          <p class="tw-select-none !tw-my-1">python</p>]]></description>
</item></channel>
</rss>
