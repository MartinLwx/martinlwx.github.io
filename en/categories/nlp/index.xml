<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>NLP - Category - MartinLwx&#39;s Blog</title>
        <link>https://martinlwx.github.io/en/categories/nlp/</link>
        <description>NLP - Category - MartinLwx&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>martinlwx@163.com (MartinLwx)</managingEditor>
            <webMaster>martinlwx@163.com (MartinLwx)</webMaster><copyright>&lt;a rel=&#34;license noopener&#34; href=&#34;https://creativecommons.org/licenses/by-nc-nd/4.0/&#34; target=&#34;_blank&#34;&gt;CC BY-NC-ND 4.0&lt;/a&gt;</copyright><lastBuildDate>Mon, 23 Dec 2024 22:01:35 &#43;0800</lastBuildDate><atom:link href="https://martinlwx.github.io/en/categories/nlp/" rel="self" type="application/rss+xml" /><item>
    <title>Reading Notes: Generalization through Memorization: Nearest Neighbor Language Models</title>
    <link>https://martinlwx.github.io/en/what-is-knn-lm/</link>
    <pubDate>Mon, 23 Dec 2024 22:01:35 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/what-is-knn-lm/</guid>
    <description><![CDATA[<h2 id="motivation" class="headerLink">
    <a href="#motivation" class="header-mark"></a>Motivation</h2><p>A language solves 2 subproblems.</p>
<ol>
<li>Mapping sentence prefixes to fixed-size representation.</li>
<li>Using these representations to predict the next token in the context.</li>
</ol>
<p>The $k\texttt{NN-LM}$ proposed in this hypothesis that <em>representation learning problem may be easier than the prediction problem</em></p>
<h2 id="knn-lm" class="headerLink">
    <a href="#knn-lm" class="header-mark"></a>kNN-LM</h2><p>The following graph demonstrates the idea behind the $k\texttt{NN-LM}$ model.</p>
<figure><img src="/img/knn-lm.png" width="resize">
</figure>

<h3 id="data-preparation" class="headerLink">
    <a href="#data-preparation" class="header-mark"></a>Data Preparation</h3><p>To use the $k\texttt{NN-LM}$, we need to preprocess the documents in the corpus. The preprocessing procedure can be divided into some steps. Take the following sentence as an example.</p>]]></description>
</item><item>
    <title>Reading Notes: In-Context Retrieval-Augmented Language Models</title>
    <link>https://martinlwx.github.io/en/in-context-ralm-paper-reading/</link>
    <pubDate>Wed, 04 Dec 2024 00:53:25 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/in-context-ralm-paper-reading/</guid>
    <description><![CDATA[<h2 id="the-idea" class="headerLink">
    <a href="#the-idea" class="header-mark"></a>The idea</h2><p>In-Context RALM<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> is the RAG technology for Autoregressive LM. In summary, the RAG technology involves using a retriever during model inference to fetch relevant documents, which are then concatenated with the origin input.</p>
<p>In the In-Context Learning setting, some examples are placed <em>before</em> the user&rsquo;s input, and then they are fed to LLM. Similarly, the In-Context RALM works in a <em>similar</em> way: it directly concatenates <em>the most relevant retrieved document in front of</em> the model&rsquo;s input. The advantage is that there&rsquo;s <em>no need to retrain the LLM</em>. A diagram created with Mermaid is shown below.</p>]]></description>
</item><item>
    <title>Reading Notes: REALM: Retrieval-Augmented Language Model Pre-Training</title>
    <link>https://martinlwx.github.io/en/rag-realm-paper-reading/</link>
    <pubDate>Sat, 30 Nov 2024 00:42:53 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/rag-realm-paper-reading/</guid>
    <description><![CDATA[<h2 id="introduction" class="headerLink">
    <a href="#introduction" class="header-mark"></a>Introduction</h2><p>Recently I was planning to learn the RAG technology so I started to read some related papers. I found a good <a href="https://acl2023-retrieval-lm.github.io/slides/3-architecture.pdf" target="_blank" rel="noopener noreferrer">Roadmap</a> in the ACL 2023 Tutorial&rsquo;s slide. Today&rsquo;s topic is the most fundamental one: Retrieval-Augmented Language Model Pre-Training (REALM).</p>
<div class="details admonition info open">
    <div class="details-summary admonition-title">
        <span class="icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z"/></svg></span>Info<span class="details-icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></span>
    </div>
    <div class="details-content">
        <div class="admonition-content"><p>The REALM framework uses the Masked LM (BERT) rather than LLM. So I assume that you <em>have a basic understanding of BERT</em> such as how to do pre-train/fine-tuning.</p>]]></description>
</item><item>
    <title>BPE Tokenization Demystified: Implementation and Examples</title>
    <link>https://martinlwx.github.io/en/the-bpe-tokenizer/</link>
    <pubDate>Thu, 24 Aug 2023 22:06:37 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/the-bpe-tokenizer/</guid>
    <description><![CDATA[<h2 id="a-taxonomy-of-tokenization-methods" class="headerLink">
    <a href="#a-taxonomy-of-tokenization-methods" class="header-mark"></a>A taxonomy of tokenization methods</h2><p>In NLP, one crux of problems is - how to tokenize the text. There are three methods available:</p>
<ul>
<li>Char-level</li>
<li>Word-level</li>
<li>Subword-level</li>
</ul>
<p>Let&rsquo;s talk about the Char-level tokenizer. That is, we tokenize the text into a char stream. <em>For instance, <code>highest -&gt; h, i, g, h, e, s, t</code></em>. One advantage of the Char-level tokenizer is that the size of Vocab won&rsquo;t be that large. The size of Vocab is equal to the size of the alphabet. So you probably won&rsquo;t meet the infamous Out-of-vocabulary(OOV) problem. However, the downside is that <strong>the char itself does not convey too much information</strong>, and <strong>we will get too many tokens after tokenizing</strong>. <em>Try to imagine that a simple word highest will give us 7 tokens</em>ðŸ˜¨</p>]]></description>
</item><item>
    <title>TF-IDF model</title>
    <link>https://martinlwx.github.io/en/an-introduction-of-tf-idf-model/</link>
    <pubDate>Wed, 16 Aug 2023 22:23:26 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/an-introduction-of-tf-idf-model/</guid>
    <description><![CDATA[<div class="details admonition info open">
    <div class="details-summary admonition-title">
        <span class="icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z"/></svg></span>Info<span class="details-icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></span>
    </div>
    <div class="details-content">
        <div class="admonition-content"><p><em>Further reading</em></p>]]></description>
</item><item>
    <title>Bag-of-Word model</title>
    <link>https://martinlwx.github.io/en/an-introduction-of-bag-of-word-model/</link>
    <pubDate>Fri, 11 Aug 2023 18:55:09 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/an-introduction-of-bag-of-word-model/</guid>
    <description><![CDATA[<h2 id="what-is-the-bag-of-word-model" class="headerLink">
    <a href="#what-is-the-bag-of-word-model" class="header-mark"></a>What is the bag-of-word model?</h2><p>In NLP, we need to represent each document as a vector because machine learning can only accept input as numbers. That is, we want to find a <em>magic</em> function that:
$$
f(\text{document}) = vector
$$</p>
<p>Today&rsquo;s topic is <strong>bag-of-word(BoW) model</strong>, which can transform a document into a vector representation.</p>
<blockquote>
  <p>ðŸ’¡ Although the BoW model is outdated in 2023, I still encourage you to learn from the history and think about some <strong>essential problems</strong>:</p>]]></description>
</item></channel>
</rss>
